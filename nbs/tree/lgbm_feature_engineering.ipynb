{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-23 11:06:10.909167: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-23 11:06:10.909425: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-23 11:06:11.013800: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-23 11:06:11.239831: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-23 11:06:12.953435: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from lightgbm import LGBMRegressor, plot_importance\n",
    "from sklearn.metrics import r2_score\n",
    "from prj.config import DATA_DIR\n",
    "from prj.data.data_loader import DataConfig, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val, weights, feature_names, cat_features=[]):   \n",
    "    cat_features_idx = [feature_names.index(f) for f in cat_features]\n",
    "    if len(cat_features_idx) > 0:\n",
    "        print(f'Using categorical features: {cat_features_idx}')\n",
    "    model.fit(X_train, y_train, feature_name=feature_names, categorical_feature=','.join([str(c) for c in cat_features_idx]))\n",
    "    pred_val = model.predict(X_val).clip(-5, 5)\n",
    "    return r2_score(y_val, pred_val, sample_weight=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': 200, 'max_depth': 3, 'num_leaves': 8, 'learning_rate': 5e-2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = {'zero_fill': False, 'ffill': False, 'include_intrastock_norm': True}\n",
    "config = DataConfig(**data_args)\n",
    "loader = DataLoader(data_dir=DATA_DIR, config=config)\n",
    "\n",
    "train_ds, val_ds = loader.load_train_and_val(start_dt=1100, end_dt=1200, val_ratio=0.2)\n",
    "\n",
    "X_train, y_train, w_train, _ = loader._build_splits(train_ds)\n",
    "X_val, y_val, w_val, _ = loader._build_splits(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.191975 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33034\n",
      "[LightGBM] [Info] Number of data points in the train set: 3016288, number of used features: 133\n",
      "[LightGBM] [Info] Start training from score -0.003544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.009591148340158062"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LGBMRegressor(**params)\n",
    "\n",
    "evaluate_model(model, X_train, y_train, X_val, y_val, w_val, loader.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using categorical features: [9, 10, 11]\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.192719 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33034\n",
      "[LightGBM] [Info] Number of data points in the train set: 3016288, number of used features: 133\n",
      "[LightGBM] [Info] Start training from score -0.003544\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.009591148340157951"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LGBMRegressor(**params)\n",
    "\n",
    "evaluate_model(model, X_train, y_train, X_val, y_val, w_val, loader.features, loader.categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = {'include_lags': True}\n",
    "config = DataConfig(**data_args)\n",
    "loader = DataLoader(data_dir=DATA_DIR, config=config)\n",
    "\n",
    "train_ds, val_ds = loader.load_train_and_val(start_dt=1100, end_dt=1200, val_ratio=0.2)\n",
    "\n",
    "X_train, y_train, w_train, _ = loader._build_splits(train_ds)\n",
    "X_val, y_val, w_val, _ = loader._build_splits(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LGBMRegressor(**params)\n",
    "\n",
    "evaluate_model(model, X_train, y_train, X_val, y_val, w_val, loader.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_args = {'include_lags': True}\n",
    "config = DataConfig(**data_args)\n",
    "loader = DataLoader(data_dir=DATA_DIR, config=config)\n",
    "\n",
    "train_ds, val_ds = loader.load_train_and_val(start_dt=1100, end_dt=1200, val_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'responder_6'\n",
    "schema = {'date_id': pl.UInt32, 'time_id': pl.UInt32, 'datetime': pl.Datetime, target: pl.Float32}\n",
    "schema.update({\n",
    "    f'{target}_knn_{n}_{persistence}_{agg}': pl.Float32 for n in knn_windows for agg in aggs\n",
    "})\n",
    "n_dates = _df['date_id'].unique().count()\n",
    "num_groups = max(0, n_dates - period + 1)\n",
    "with tqdm(total=int(num_groups)) as pbar:\n",
    "    _df = _df.sort('datetime').group_by_dynamic(\n",
    "        pl.col('date_id').cast(pl.Int64),\n",
    "        period=f\"{period}i\",\n",
    "        every=\"1i\",\n",
    "        closed='both',\n",
    "    ).map_groups(\n",
    "        wrapper_pbar(\n",
    "            pbar, \n",
    "            lambda x: _knn_features(x, period=period, knn_windows=knn_windows, features=features, target=target, schema=schema, persistence=persistence, aggs=aggs)\n",
    "        ),\n",
    "        schema=schema\n",
    "    ).sort('datetime').drop(target, 'date_id', 'time_id')\n",
    "\n",
    "df = df.join(\n",
    "    _df, on='datetime', how='left'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

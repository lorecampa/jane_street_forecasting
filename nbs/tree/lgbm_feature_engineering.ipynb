{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-04 09:05:22.385466: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-04 09:05:22.385500: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-04 09:05:22.386598: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-04 09:05:22.393213: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-04 09:05:23.278447: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from lightgbm import LGBMRegressor, plot_importance\n",
    "from sklearn.metrics import r2_score\n",
    "from prj.config import DATA_DIR\n",
    "from prj.data.data_loader import DataConfig, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val, weights, feature_names, cat_features=[]):   \n",
    "    cat_features_idx = [feature_names.index(f) for f in cat_features]\n",
    "    if len(cat_features_idx) > 0:\n",
    "        print(f'Using categorical features: {cat_features_idx}')\n",
    "    model.fit(X_train, y_train, feature_name=feature_names, categorical_feature=','.join([str(c) for c in cat_features_idx]))\n",
    "    pred_val = model.predict(X_val).clip(-5, 5)\n",
    "    return r2_score(y_val, pred_val, sample_weight=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'n_estimators': 200, 'max_depth': 3, 'num_leaves': 8, 'learning_rate': 5e-2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.448876 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19354\n",
      "[LightGBM] [Info] Number of data points in the train set: 6140024, number of used features: 79\n",
      "[LightGBM] [Info] Start training from score 0.001341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.004925303885731869"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args = {}\n",
    "config = DataConfig(**data_args)\n",
    "loader = DataLoader(data_dir=DATA_DIR, config=config)\n",
    "\n",
    "complete_ds = loader.load_with_partition(8, 9)\n",
    "train_ds = complete_ds.filter(pl.col('partition_id').eq(8))\n",
    "val_ds = complete_ds.filter(pl.col('partition_id').eq(9))\n",
    "\n",
    "X_train, y_train, w_train, _ = loader._build_splits(train_ds)\n",
    "X_val, y_val, w_val, _ = loader._build_splits(val_ds)\n",
    "\n",
    "model = LGBMRegressor(**params)\n",
    "\n",
    "evaluate_model(model, X_train, y_train, X_val, y_val, w_val, loader.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.467964 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19393\n",
      "[LightGBM] [Info] Number of data points in the train set: 6140024, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.001341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.005006532263017727"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args = {'include_symbol_id': True}\n",
    "config = DataConfig(**data_args)\n",
    "loader = DataLoader(data_dir=DATA_DIR, config=config)\n",
    "\n",
    "complete_ds = loader.load_with_partition(8, 9)\n",
    "train_ds = complete_ds.filter(pl.col('partition_id').eq(8))\n",
    "val_ds = complete_ds.filter(pl.col('partition_id').eq(9))\n",
    "\n",
    "X_train, y_train, w_train, _ = loader._build_splits(train_ds)\n",
    "X_val, y_val, w_val, _ = loader._build_splits(val_ds)\n",
    "\n",
    "model = LGBMRegressor(**params)\n",
    "\n",
    "evaluate_model(model, X_train, y_train, X_val, y_val, w_val, loader.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.353123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19648\n",
      "[LightGBM] [Info] Number of data points in the train set: 6140024, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.001341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.005115153483336354"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args = {'include_time_id': True, 'include_symbol_id': True}\n",
    "config = DataConfig(**data_args)\n",
    "loader = DataLoader(data_dir=DATA_DIR, config=config)\n",
    "\n",
    "complete_ds = loader.load_with_partition(8, 9)\n",
    "train_ds = complete_ds.filter(pl.col('partition_id').eq(8))\n",
    "val_ds = complete_ds.filter(pl.col('partition_id').eq(9))\n",
    "\n",
    "X_train, y_train, w_train, _ = loader._build_splits(train_ds)\n",
    "X_val, y_val, w_val, _ = loader._build_splits(val_ds)\n",
    "\n",
    "model = LGBMRegressor(**params)\n",
    "\n",
    "evaluate_model(model, X_train, y_train, X_val, y_val, w_val, loader.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.250700 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19609\n",
      "[LightGBM] [Info] Number of data points in the train set: 6140024, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.001341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.005132033405778502"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args = {'include_time_id': True}\n",
    "config = DataConfig(**data_args)\n",
    "loader = DataLoader(data_dir=DATA_DIR, config=config)\n",
    "\n",
    "complete_ds = loader.load_with_partition(8, 9)\n",
    "train_ds = complete_ds.filter(pl.col('partition_id').eq(8))\n",
    "val_ds = complete_ds.filter(pl.col('partition_id').eq(9))\n",
    "\n",
    "X_train, y_train, w_train, _ = loader._build_splits(train_ds)\n",
    "X_val, y_val, w_val, _ = loader._build_splits(val_ds)\n",
    "\n",
    "model = LGBMRegressor(**params)\n",
    "\n",
    "evaluate_model(model, X_train, y_train, X_val, y_val, w_val, loader.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [00:21<00:00, 16.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.524577 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33418\n",
      "[LightGBM] [Info] Number of data points in the train set: 6140024, number of used features: 135\n",
      "[LightGBM] [Info] Start training from score 0.001341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.005789860094759236"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args = {'include_time_id': True, 'include_intrastock_norm_temporal': True, 'include_symbol_id': True}  \n",
    "config = DataConfig(**data_args)\n",
    "loader = DataLoader(data_dir=DATA_DIR, config=config)\n",
    "\n",
    "complete_ds = loader.load_with_partition(8, 9)\n",
    "train_ds = complete_ds.filter(pl.col('partition_id').eq(8))\n",
    "val_ds = complete_ds.filter(pl.col('partition_id').eq(9))\n",
    "\n",
    "X_train, y_train, w_train, _ = loader._build_splits(train_ds)\n",
    "X_val, y_val, w_val, _ = loader._build_splits(val_ds)\n",
    "\n",
    "model = LGBMRegressor(**params)\n",
    "\n",
    "evaluate_model(model, X_train, y_train, X_val, y_val, w_val, loader.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 346/346 [00:20<00:00, 16.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.293371 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 33379\n",
      "[LightGBM] [Info] Number of data points in the train set: 6140024, number of used features: 134\n",
      "[LightGBM] [Info] Start training from score 0.001341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.005880807434581414"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_args = {'include_time_id': True, 'include_intrastock_norm_temporal': True, 'include_symbol_id': False}  \n",
    "config = DataConfig(**data_args)\n",
    "loader = DataLoader(data_dir=DATA_DIR, config=config)\n",
    "\n",
    "complete_ds = loader.load_with_partition(8, 9)\n",
    "train_ds = complete_ds.filter(pl.col('partition_id').eq(8))\n",
    "val_ds = complete_ds.filter(pl.col('partition_id').eq(9))\n",
    "\n",
    "X_train, y_train, w_train, _ = loader._build_splits(train_ds)\n",
    "X_val, y_val, w_val, _ = loader._build_splits(val_ds)\n",
    "\n",
    "model = LGBMRegressor(**params)\n",
    "\n",
    "evaluate_model(model, X_train, y_train, X_val, y_val, w_val, loader.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

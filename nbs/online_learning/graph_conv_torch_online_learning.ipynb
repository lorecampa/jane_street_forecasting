{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-07T16:43:18.101283Z",
     "iopub.status.busy": "2025-01-07T16:43:18.100944Z",
     "iopub.status.idle": "2025-01-07T16:43:25.885197Z",
     "shell.execute_reply": "2025-01-07T16:43:25.884420Z",
     "shell.execute_reply.started": "2025-01-07T16:43:18.101254Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional, Tuple, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "import kaggle_evaluation.jane_street_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:43:25.886712Z",
     "iopub.status.busy": "2025-01-07T16:43:25.886330Z",
     "iopub.status.idle": "2025-01-07T16:43:25.894995Z",
     "shell.execute_reply": "2025-01-07T16:43:25.893995Z",
     "shell.execute_reply.started": "2025-01-07T16:43:25.886686Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiStockGraphDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset: pl.LazyFrame, adjacency_matrices: np.ndarray, stock_ids: list):\n",
    "        self.dataset = dataset\n",
    "        self.adjacency_matrices = adjacency_matrices\n",
    "        self.stock_ids = stock_ids\n",
    "        self.num_stocks = len(self.stock_ids)\n",
    "        self.dataset_len = self.dataset.select(['date_id', 'time_id']).unique().shape[0]\n",
    "        self._load()\n",
    "    \n",
    "    def _load(self):\n",
    "        all_combinations = (\n",
    "            self.dataset.select(['date_id', 'time_id'])\n",
    "            .unique()\n",
    "            .join(pl.DataFrame({'symbol_id': self.stock_ids}, \n",
    "                               schema={'symbol_id': pl.Int8}), how=\"cross\")\n",
    "        )\n",
    "        feature_cols = [f'feature_{i:02d}' for i in range(79)]\n",
    "        self.batch = (\n",
    "            all_combinations\n",
    "            .join(self.dataset.with_columns(pl.lit(1).alias('mask')), \n",
    "                  on=['date_id', 'time_id', 'symbol_id'], how=\"left\")\n",
    "            .fill_null(0)  # fill all columns with 0 for missing stocks (including the mask)\n",
    "            .sort(['date_id', 'time_id', 'symbol_id'])\n",
    "        )\n",
    "        # num_stocks rows for each date and time\n",
    "        self.X = self.batch.select(feature_cols).to_numpy().astype(np.float32)\n",
    "        self.y = self.batch.select(['responder_6']).to_numpy().flatten().astype(np.float32)\n",
    "        self.s = self.batch.select(['symbol_id']).to_numpy().flatten().astype(np.int32)\n",
    "        self.date_ids = self.batch.select(['date_id']).to_numpy().flatten()\n",
    "        self.masks = self.batch.select(['mask']).to_numpy().flatten() == 0\n",
    "        self.weights = self.batch.select(['weight']).to_numpy().flatten().astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_row = idx * self.num_stocks\n",
    "        features = self.X[start_row:start_row+self.num_stocks, :]\n",
    "        targets = self.y[start_row:start_row+self.num_stocks]\n",
    "        masks = self.masks[start_row:start_row+self.num_stocks]\n",
    "        weights = self.weights[start_row:start_row+self.num_stocks]\n",
    "        symbols = self.s[start_row:start_row+self.num_stocks]\n",
    "\n",
    "        date_id = self.date_ids[start_row]\n",
    "        adj_matrix = self.adjacency_matrices[date_id]\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(features), \n",
    "            torch.tensor(targets), \n",
    "            torch.tensor(masks), \n",
    "            torch.tensor(weights), \n",
    "            torch.tensor(symbols),\n",
    "            torch.tensor(adj_matrix, dtype=torch.int)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:43:25.896262Z",
     "iopub.status.busy": "2025-01-07T16:43:25.896030Z",
     "iopub.status.idle": "2025-01-07T16:43:25.925219Z",
     "shell.execute_reply": "2025-01-07T16:43:25.923954Z",
     "shell.execute_reply.started": "2025-01-07T16:43:25.896215Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransposeLayer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        return input.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:43:25.926500Z",
     "iopub.status.busy": "2025-01-07T16:43:25.926182Z",
     "iopub.status.idle": "2025-01-07T16:43:25.944211Z",
     "shell.execute_reply": "2025-01-07T16:43:25.943298Z",
     "shell.execute_reply.started": "2025-01-07T16:43:25.926472Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class WeightedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedMSELoss, self).__init__()\n",
    "    \n",
    "    def forward(self, predictions: Tensor, targets: Tensor, weights: Tensor) -> Tensor:\n",
    "        squared_diff = (predictions - targets) ** 2\n",
    "        weighted_squared_diff = weights * squared_diff\n",
    "        return weighted_squared_diff.sum() / weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:43:25.945265Z",
     "iopub.status.busy": "2025-01-07T16:43:25.945032Z",
     "iopub.status.idle": "2025-01-07T16:43:25.956540Z",
     "shell.execute_reply": "2025-01-07T16:43:25.955322Z",
     "shell.execute_reply.started": "2025-01-07T16:43:25.945221Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GraphConvEncoderLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim, dim_feedforward_mult=4, dropout_rate=0.1):\n",
    "        super(GraphConvEncoderLayer, self).__init__()\n",
    "        \n",
    "        self.graph_conv = GCNConv(\n",
    "            in_channels=hidden_dim, \n",
    "            out_channels=hidden_dim\n",
    "        )\n",
    "\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim * dim_feedforward_mult),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim * dim_feedforward_mult, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_dim)\n",
    "        self.norm2 = nn.LayerNorm(hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        batch_size, num_nodes, num_features = x.size()\n",
    "\n",
    "        residual = x\n",
    "        x = x.reshape(batch_size * num_nodes, num_features)\n",
    "        x = self.graph_conv(x, edge_index)\n",
    "        x = x.reshape(batch_size, num_nodes, num_features)        \n",
    "        x = self.dropout1(x) + residual\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        residual = x\n",
    "        x = self.feedforward(x)\n",
    "        x = self.dropout2(x) + residual\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:43:25.957527Z",
     "iopub.status.busy": "2025-01-07T16:43:25.957307Z",
     "iopub.status.idle": "2025-01-07T16:43:25.980770Z",
     "shell.execute_reply": "2025-01-07T16:43:25.979637Z",
     "shell.execute_reply.started": "2025-01-07T16:43:25.957506Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GraphConvEncoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, dim_feedforward_mult=4, dropout_rate=0.1):\n",
    "        super(GraphConvEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            GraphConvEncoderLayer(\n",
    "                hidden_dim=hidden_dim,\n",
    "                dim_feedforward_mult=dim_feedforward_mult,\n",
    "                dropout_rate=dropout_rate\n",
    "            ) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        batch_size, num_nodes, _ = x.size()\n",
    "\n",
    "        edge_indices = []\n",
    "        for batch_idx in range(batch_size):\n",
    "            adj_matrix = adj[batch_idx]\n",
    "            src, tgt = torch.nonzero(adj_matrix, as_tuple=True)\n",
    "            src = src + batch_idx * num_nodes\n",
    "            tgt = tgt + batch_idx * num_nodes\n",
    "            edge_indices.append(torch.stack([src, tgt], dim=0))\n",
    "\n",
    "        edge_index = torch.cat(edge_indices, dim=1).to(x.device)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:43:25.983573Z",
     "iopub.status.busy": "2025-01-07T16:43:25.983295Z",
     "iopub.status.idle": "2025-01-07T16:43:26.007782Z",
     "shell.execute_reply": "2025-01-07T16:43:26.006646Z",
     "shell.execute_reply.started": "2025-01-07T16:43:25.983549Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class StockGCNModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_features,\n",
    "        hidden_dim=64,\n",
    "        output_dim=1,\n",
    "        num_layers=2,\n",
    "        num_stocks=39,\n",
    "        embedding_dim=16,\n",
    "        use_embeddings=False,\n",
    "        dropout_rate=0.2,\n",
    "        dim_feedforward_mult=4,\n",
    "    ):\n",
    "        super(StockGCNModel, self).__init__()\n",
    "\n",
    "        self.use_embeddings = use_embeddings\n",
    "\n",
    "        self.init_layers = nn.Sequential(\n",
    "            # TransposeLayer(),\n",
    "            # nn.BatchNorm1d(input_features),\n",
    "            # TransposeLayer(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        )\n",
    "        self.feature_projector = []\n",
    "        if use_embeddings:\n",
    "            self.feature_projector.append(nn.Linear(input_features + embedding_dim, hidden_dim))\n",
    "            self.embedding_layer = nn.Embedding(num_stocks, embedding_dim)\n",
    "        else:\n",
    "            self.feature_projector.append(nn.Linear(input_features, hidden_dim))\n",
    "        self.feature_projector += [\n",
    "            # TransposeLayer(),\n",
    "            # nn.BatchNorm1d(hidden_dim),\n",
    "            # TransposeLayer(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "        ]\n",
    "        self.feature_projector = nn.Sequential(*self.feature_projector)\n",
    "\n",
    "        self.encoder = GraphConvEncoder(\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dim_feedforward_mult=dim_feedforward_mult,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            # TransposeLayer(),\n",
    "            # nn.BatchNorm1d(hidden_dim),\n",
    "            # TransposeLayer(),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, symbols, adj):\n",
    "        batch_size, num_stocks, num_features = x.size()\n",
    "\n",
    "        x = self.init_layers(x)\n",
    "        if self.use_embeddings:\n",
    "            stock_embeddings = self.embedding_layer(symbols)\n",
    "            x = torch.cat([x, stock_embeddings], dim=-1)\n",
    "        x = self.feature_projector(x)\n",
    "        x = self.encoder(x, adj)\n",
    "\n",
    "        output = self.predictor(x)\n",
    "        return 5 * torch.tanh(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:43:26.009269Z",
     "iopub.status.busy": "2025-01-07T16:43:26.009025Z",
     "iopub.status.idle": "2025-01-07T16:43:26.034264Z",
     "shell.execute_reply": "2025-01-07T16:43:26.033334Z",
     "shell.execute_reply.started": "2025-01-07T16:43:26.009251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_correlation_from_pivot(pivot_df):\n",
    "    correlations = (\n",
    "        pivot_df\n",
    "        .drop(['date_id', 'time_id'])\n",
    "        .corr()\n",
    "        .fill_nan(0).fill_null(0)\n",
    "    )\n",
    "    order = [int(i) for i in correlations.columns]\n",
    "    new_order = np.argsort(order).tolist()\n",
    "    columns_order = [str(i) for i in np.array(order)[new_order].tolist()]\n",
    "    correlations = correlations[columns_order]\n",
    "    correlations = correlations[new_order, :]\n",
    "    return np.abs(correlations.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:43:26.035219Z",
     "iopub.status.busy": "2025-01-07T16:43:26.035023Z",
     "iopub.status.idle": "2025-01-07T16:43:26.056182Z",
     "shell.execute_reply": "2025-01-07T16:43:26.055317Z",
     "shell.execute_reply.started": "2025-01-07T16:43:26.035201Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:43:26.057258Z",
     "iopub.status.busy": "2025-01-07T16:43:26.057061Z",
     "iopub.status.idle": "2025-01-07T16:43:26.209251Z",
     "shell.execute_reply": "2025-01-07T16:43:26.208319Z",
     "shell.execute_reply.started": "2025-01-07T16:43:26.057241Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StockGCNModel(\n",
       "  (init_layers): Sequential(\n",
       "    (0): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (feature_projector): Sequential(\n",
       "    (0): Linear(in_features=79, out_features=64, bias=True)\n",
       "    (1): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (encoder): GraphConvEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): GraphConvEncoderLayer(\n",
       "        (graph_conv): GCNConv(64, 64)\n",
       "        (feedforward): Sequential(\n",
       "          (0): Linear(in_features=64, out_features=256, bias=True)\n",
       "          (1): SiLU()\n",
       "          (2): Dropout(p=0.2, inplace=False)\n",
       "          (3): Linear(in_features=256, out_features=64, bias=True)\n",
       "        )\n",
       "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.2, inplace=False)\n",
       "        (dropout2): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (predictor): Sequential(\n",
       "    (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (1): SiLU()\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = '/home/lorecampa/projects/jane_street_forecasting/dataset/models/graph_conv/model_3_7.pth'\n",
    "model = StockGCNModel(\n",
    "    input_features=79,\n",
    "    output_dim=1,\n",
    "    num_layers=1,\n",
    "    dropout_rate=0.2,\n",
    "    dim_feedforward_mult=4,\n",
    "    hidden_dim=64)\n",
    "model.load_state_dict(torch.load(save_path, weights_only=True, map_location=torch.device(device)))\n",
    "model = model.to(device)\n",
    "inference_model = StockGCNModel(\n",
    "    input_features=79,\n",
    "    output_dim=1,\n",
    "    num_layers=1,\n",
    "    dropout_rate=0.2,\n",
    "    dim_feedforward_mult=4,\n",
    "    hidden_dim=64)\n",
    "inference_model.load_state_dict(torch.load(save_path, weights_only=True, map_location=torch.device(device)))\n",
    "inference_model = inference_model.to(device)\n",
    "\n",
    "loss_fn = WeightedMSELoss()\n",
    "inference_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:43:26.210523Z",
     "iopub.status.busy": "2025-01-07T16:43:26.210184Z",
     "iopub.status.idle": "2025-01-07T16:43:26.216476Z",
     "shell.execute_reply": "2025-01-07T16:43:26.215616Z",
     "shell.execute_reply.started": "2025-01-07T16:43:26.210500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from prj.config import DATA_DIR\n",
    "\n",
    "# FINE_TUNING = True\n",
    "# N_EPOCHS_PER_TRAIN_MAX = 10\n",
    "# BATCH_SIZE = 2048\n",
    "# OLD_DATA_FRACTION = 0.1\n",
    "# FEATURE_COLS = [f'feature_{i:02d}' for i in range(79)]\n",
    "# GRADIENT_CLIPPING = 10\n",
    "# EARLY_STOPPING_DAYS = 7\n",
    "# ES_PATIENCE = 7\n",
    "# TRAIN_EVERY = 23\n",
    "# date_idx = -1\n",
    "# epoch = None\n",
    "# best_epoch = None\n",
    "# best_score = None\n",
    "# train_dataloader, val_dataloader, train_iterator, val_iterator = None, None, None, None\n",
    "# save_path = './best_model.pth'\n",
    "# acc_metrics = dict(ss_res=0.0, ss_tot=0.0)\n",
    "# start_train = False\n",
    "# is_training_loop = False\n",
    "\n",
    "# gradient_clipping_decay = 0.5\n",
    "# gradient_clipping = GRADIENT_CLIPPING * gradient_clipping_decay\n",
    "# lr_decay = 0.7\n",
    "# lr = 1e-5\n",
    "# optimizer = None\n",
    "\n",
    "# TIME_LIMIT = 30\n",
    "# MAX_FINE_TUNING_TIME_LIMIT = time.time() + 60 * 60 * 8 # after 8 hours, stop all the online learning\n",
    "\n",
    "# FEATURES = [f'feature_{i:02d}' for i in range(79)]\n",
    "# COLUMNS = FEATURES + ['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']\n",
    "# BASE_PATH = DATA_DIR / 'train.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-10 11:58:26.987771: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-10 11:58:26.987805: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-10 11:58:26.989056: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-10 11:58:26.995704: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-10 11:58:27.757598: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from prj.data.data_loader import DataLoader as PrjDataLoader\n",
    "from prj.data.data_loader import DataConfig as PrjDataConfig\n",
    "\n",
    "config = PrjDataConfig(**{})\n",
    "loader = PrjDataLoader(data_dir=DATA_DIR, config=config)\n",
    "start, end = 1360, 1529\n",
    "# start, end = 1360, 1370\n",
    "\n",
    "# start, end = 1190, 1200\n",
    "test_ds = loader.load(start-1, end).collect()\n",
    "\n",
    "y_test = test_ds.filter(pl.col('date_id').ge(start))['responder_6'].to_numpy().flatten()\n",
    "w_test = test_ds.filter(pl.col('date_id').ge(start))['weight'].to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df: pl.LazyFrame | pl.DataFrame, data_stats_dict: dict, features: list[str], eps=1e-9) -> pl.LazyFrame:\n",
    "    cat_features = ['feature_09', 'feature_10', 'feature_11']\n",
    "    features = [f for f in features if f not in cat_features]\n",
    "        \n",
    "    eps = 1e-8\n",
    "    return df.with_columns(\n",
    "        [(pl.col(col).sub(data_stats_dict[f'{col}_mean'])).truediv(data_stats_dict[f'{col}_std']).add(eps) for col in features]\n",
    "    ).with_columns(\n",
    "        pl.col(f).truediv(data_stats_dict[f'{f}_max']) for f in cat_features\n",
    "    )\n",
    "    \n",
    "data_stats_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphConvTrainerConfig:\n",
    "    LAST_TRAIN_DATE = 1698\n",
    "    INITIAL_ES_DAYS = 30\n",
    "    CORRELATION_THR = 0.1\n",
    "    WINDOW_LEN = 2 if not os.getenv('KAGGLE_IS_COMPETITION_RERUN') else 7\n",
    "    \n",
    "    FINE_TUNING = True\n",
    "    N_EPOCHS_PER_TRAIN_MAX = 30\n",
    "    BATCH_SIZE = 2048\n",
    "    OLD_DATA_FRACTION = 0.1\n",
    "    FEATURE_COLS = [f'feature_{i:02d}' for i in range(79)]\n",
    "    GRADIENT_CLIPPING = 10\n",
    "    EARLY_STOPPING_DAYS = 7\n",
    "    ES_PATIENCE = 3\n",
    "    TRAIN_EVERY = 23\n",
    "    TIME_LIMIT = 30\n",
    "    MAX_FINE_TUNING_TIME_LIMIT = time.time() + 60 * 60 * 8 # after 8 hours, stop all the online learning\n",
    "\n",
    "    FEATURES = [f'feature_{i:02d}' for i in range(79)]\n",
    "    COLUMNS = FEATURES + ['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']\n",
    "\n",
    "\n",
    "class GraphConvTrainer:\n",
    "    \n",
    "    def __init__(self):\n",
    "    \n",
    "        self.config = GraphConvTrainerConfig()\n",
    "        self.current_day_data : pl.DataFrame | None = None\n",
    "        \n",
    "        \n",
    "        config = PrjDataConfig()\n",
    "        loader = PrjDataLoader(data_dir=DATA_DIR, config=config)\n",
    "        self.old_dataset = loader.load(self.config.LAST_TRAIN_DATE - 60, self.config.LAST_TRAIN_DATE)\\\n",
    "            .sort(['date_id', 'time_id', 'symbol_id']) \\\n",
    "            .fill_nan(None) \\\n",
    "            .fill_null(strategy='zero') \\\n",
    "            .select(self.config.COLUMNS) \\\n",
    "            .pipe(\n",
    "                standardize,\n",
    "                data_stats_dict=data_stats_dict,\n",
    "                features=self.config.FEATURES,\n",
    "            )\n",
    "            \n",
    "        self.new_dataset = self.old_dataset.filter(pl.col('date_id') > self.config.LAST_TRAIN_DATE - self.config.INITIAL_ES_DAYS).collect()\n",
    "        if OLD_DATA_FRACTION > 0:\n",
    "            self.old_dataset = self.old_dataset.filter(pl.col('date_id') <= self.config.LAST_TRAIN_DATE - self.config.INITIAL_ES_DAYS).collect()\n",
    "        else:\n",
    "            self.old_dataset = None\n",
    "            \n",
    "        \n",
    "        past_responders_pivot: pl.DataFrame | None = None\n",
    "        current_date_id = -1\n",
    "        current_stock_ids = list(range(39))\n",
    "        num_dates = 0\n",
    "\n",
    "        adjacency_matrices = np.load('/kaggle/input/jane-street-2024-graph-computation/adjacency_matrices.npy')\n",
    "        current_corr_matrix = np.load('/kaggle/input/jane-street-2024-graph-computation/correlations.npy')[-1, :, :]\n",
    "        \n",
    "        self.date_idx = -1\n",
    "        self.epoch = None\n",
    "        self.best_epoch = None\n",
    "        self.best_score = None\n",
    "        self.train_dataloader, self.val_dataloader, self.train_iterator, self.val_iterator = None, None, None, None\n",
    "        self.save_path = '/kaggle/working/best_model.pth'\n",
    "        self.acc_metrics = dict(ss_res=0.0, ss_tot=0.0)\n",
    "        self.start_train = False\n",
    "        self.is_training_loop = False\n",
    "\n",
    "        gradient_clipping_decay = 0.5\n",
    "        self.gradient_clipping = self.config.GRADIENT_CLIPPING * gradient_clipping_decay\n",
    "        self.lr_decay = 0.7\n",
    "        self.lr = 1e-5\n",
    "        self.optimizer = None\n",
    "        \n",
    "    def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "        \n",
    "        initial_time = time.time()\n",
    "        FINE_TUNING = FINE_TUNING & (initial_time < MAX_FINE_TUNING_TIME_LIMIT)\n",
    "        start_train = start_train if FINE_TUNING else False\n",
    "\n",
    "        test = test.pipe(\n",
    "            standardize,\n",
    "            data_stats_dict=data_stats_dict,\n",
    "            features=FEATURES,\n",
    "        )\n",
    "\n",
    "        if lags is not None:\n",
    "            print(f\"Date id: {test['date_id'].min()}\")\n",
    "            lags_ = lags.select(\n",
    "                pl.col('date_id').sub(1),\n",
    "                pl.col(['time_id', 'symbol_id']),\n",
    "                pl.col('responder_6_lag_1').alias('responder_6'),\n",
    "            )\n",
    "            if current_day_data is not None:\n",
    "                current_day_data = current_day_data.join(lags_, on=['date_id', 'time_id', 'symbol_id'], \n",
    "                                                        how='left').fill_null(0)\n",
    "                current_day_data = current_day_data.select(COLUMNS)\n",
    "                current_day_data = (\n",
    "                    current_day_data\n",
    "                    .drop('date_id')\n",
    "                    .with_columns(pl.lit(last_train_date + date_idx + 1).cast(pl.Int16).alias('date_id'))\n",
    "                    .select(COLUMNS)\n",
    "                )\n",
    "\n",
    "\n",
    "                new_dataset = new_dataset.vstack(current_day_data)\n",
    "                last_adj = current_corr_matrix.copy()\n",
    "                \n",
    "                last_adj[np.arange(len(current_stock_ids)), np.arange(len(current_stock_ids))] = 0\n",
    "                last_adj = (last_adj > CORRELATION_THR).astype(np.int32)[np.newaxis, :, :]\n",
    "                adjacency_matrices = np.concatenate([adjacency_matrices, last_adj], axis=0)\n",
    "                \n",
    "            current_day_data = test\n",
    "\n",
    "            all_combinations = (\n",
    "                lags_.select(['date_id', 'time_id'])\n",
    "                .unique()\n",
    "                .join(pl.DataFrame({'symbol_id': current_stock_ids}, \n",
    "                                schema={'symbol_id': pl.Int8}), how=\"cross\")\n",
    "            )\n",
    "            \n",
    "            pivot_lags = (\n",
    "                all_combinations\n",
    "                .join(lags_, on=['date_id', 'time_id', 'symbol_id'], how=\"left\")\n",
    "                .fill_null(0)\n",
    "                .sort(['date_id', 'time_id', 'symbol_id'])\n",
    "                .pivot(index=['date_id', 'time_id'], values='responder_6', on='symbol_id')\n",
    "                .fill_null(0)\n",
    "            )\n",
    "            \n",
    "            past_responders_pivot = (\n",
    "                pl.concat([past_responders_pivot, pivot_lags], how='diagonal')\n",
    "                .filter(pl.col('date_id') >= current_date_id - WINDOW_LEN - 1)\n",
    "            ) if past_responders_pivot is not None else pivot_lags\n",
    "            \n",
    "            if num_dates >= WINDOW_LEN:\n",
    "                current_corr_matrix = compute_correlation_from_pivot(past_responders_pivot)\n",
    "\n",
    "            if FINE_TUNING and not start_train:\n",
    "                start_train = (date_idx + 1) % TRAIN_EVERY == 0\n",
    "                if start_train:\n",
    "                    print('Starting new fine tuning')\n",
    "                    model.eval()\n",
    "                    max_date = new_dataset.select(pl.col('date_id').max()).item()\n",
    "                    new_validation_dataset = new_dataset.filter(pl.col('date_id') > max_date - EARLY_STOPPING_DAYS)\n",
    "                    new_training_dataset = new_dataset.filter(pl.col('date_id') <= max_date - EARLY_STOPPING_DAYS)\n",
    "                    train_days = new_training_dataset['date_id'].unique().sort().to_list()\n",
    "                    val_days = new_validation_dataset['date_id'].unique().sort().to_list()\n",
    "                    print(f'Training days: {train_days}')\n",
    "                    print(f'Validation days: {val_days}')\n",
    "                    \n",
    "                    if OLD_DATA_FRACTION > 0:\n",
    "                        old_data_len = OLD_DATA_FRACTION * new_training_dataset.shape[0] / (1 - OLD_DATA_FRACTION)\n",
    "                        time_factions = min(1, old_data_len / old_dataset.shape[0])\n",
    "                        old_date_times = old_dataset.select(['date_id', 'time_id']).unique().sample(fraction=time_factions)\n",
    "                                            \n",
    "                        old_training_dataset = old_dataset.join(old_date_times, on=['date_id', 'time_id'], how='inner')\n",
    "                        \n",
    "                        print(f'Old training days: {old_training_dataset[\"date_id\"].unique().to_list()}')\n",
    "                        \n",
    "                        train_dataloader = MultiStockGraphDataset(pl.concat([old_training_dataset, new_training_dataset]), adjacency_matrices.copy(), current_stock_ids)\n",
    "                        val_dataloader = MultiStockGraphDataset(new_validation_dataset, adjacency_matrices.copy(), current_stock_ids)\n",
    "                    else:\n",
    "                        train_dataloader = MultiStockGraphDataset(new_training_dataset, adjacency_matrices.copy(), current_stock_ids)\n",
    "                        val_dataloader = MultiStockGraphDataset(new_validation_dataset, adjacency_matrices.copy(), current_stock_ids)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "                    train_dataloader = DataLoader(train_dataloader, shuffle=True, batch_size=BATCH_SIZE, num_workers=0)\n",
    "                    val_dataloader = DataLoader(val_dataloader, shuffle=False, batch_size=2048, num_workers=0)\n",
    "                    val_iterator = iter(val_dataloader)\n",
    "                    acc_metrics = dict(ss_res=0.0, ss_tot=0.0)\n",
    "                    is_training_loop = False\n",
    "                    epoch = -1\n",
    "                    best_epoch = -1\n",
    "                    best_score = -1e10\n",
    "\n",
    "                    if OLD_DATA_FRACTION > 0:\n",
    "                        max_new_date_id = new_training_dataset['date_id'].max()\n",
    "                        old_dataset = old_dataset.vstack(new_training_dataset).filter(\n",
    "                            pl.col('date_id').is_between(max_new_date_id - 30, max_new_date_id)\n",
    "                        )\n",
    "                        \n",
    "                    new_dataset = new_validation_dataset\n",
    "                    \n",
    "            date_idx += 1\n",
    "        else:\n",
    "            current_day_data = current_day_data.vstack(test)\n",
    "            \n",
    "        if FINE_TUNING:\n",
    "            while start_train and time.time() - initial_time < TIME_LIMIT:\n",
    "                if is_training_loop:\n",
    "                    try:\n",
    "                        batch = next(train_iterator)\n",
    "                    except StopIteration:\n",
    "                        model.eval()\n",
    "                        val_iterator = iter(val_dataloader)\n",
    "                        acc_metrics = dict(ss_res=0.0, ss_tot=0.0)\n",
    "                        is_training_loop = False\n",
    "                        continue\n",
    "            \n",
    "                    x, targets, m, w, s, A = batch\n",
    "                    optimizer.zero_grad()\n",
    "                    y_out = model.forward(x.to(device), s.to(device), A.to(device)).squeeze()\n",
    "                    loss = loss_fn(y_out, targets.to(device), w.to(device))\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                else:\n",
    "                    try:\n",
    "                        batch = next(val_iterator)\n",
    "                    except StopIteration:\n",
    "                        score = 1 - acc_metrics['ss_res'] / acc_metrics['ss_tot']\n",
    "                        print(f'Epoch {epoch} completed with score {score}')\n",
    "                        epoch += 1\n",
    "                        if score > best_score:\n",
    "                            torch.save(model.state_dict(), save_path)\n",
    "                            inference_model.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "                            inference_model.to(device)\n",
    "                            inference_model.eval()\n",
    "                            best_epoch = epoch\n",
    "                            best_score = score\n",
    "                        if epoch - best_epoch >= ES_PATIENCE or epoch == N_EPOCHS_PER_TRAIN_MAX:\n",
    "                            print(f'Stopping after {epoch} epochs')\n",
    "                            print(f'Completed Fine Tuning at time {test.select(pl.col(\"time_id\").first()).item()}')\n",
    "                            model.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "                            model.to(device)\n",
    "                            model.eval()\n",
    "                            start_train = False\n",
    "                            lr *= lr_decay\n",
    "                            gradient_clipping *= gradient_clipping_decay\n",
    "                            break\n",
    "                        model.train()\n",
    "                        train_iterator = iter(train_dataloader)\n",
    "                        is_training_loop = True\n",
    "                        continue\n",
    "\n",
    "                    x, targets, m, w, s, A = batch\n",
    "                    with torch.no_grad():\n",
    "                        y_out = model(x.to(device), s.to(device), A.to(device)).squeeze()\n",
    "                    w = w.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    acc_metrics['ss_res'] += (w * (y_out - targets) ** 2).sum().cpu()\n",
    "                    acc_metrics['ss_tot'] += (w * (targets ** 2)).sum().cpu()\n",
    "\n",
    "        if test.select(pl.col('is_scored').cast(pl.Int8).first()).item() > 0:\n",
    "            stock_ids = test.select('symbol_id').to_numpy().flatten().tolist()\n",
    "            missing_ids = [x for x in current_stock_ids if x not in stock_ids]\n",
    "            new_ids = [x for x in stock_ids if x not in current_stock_ids]\n",
    "            predict_df = test.select([f'feature_{i:02d}' for i in range(79)] + ['symbol_id']).fill_null(0).fill_nan(0)\n",
    "            predict_df = predict_df.with_columns(pl.lit(1).cast(pl.Int8).alias('valid'))\n",
    "            if len(missing_ids) > 0:\n",
    "                if test.select(pl.col('time_id').first()).item() == 0:\n",
    "                    print(f'Missing ids: {missing_ids}')\n",
    "                add_df = []\n",
    "                for missing_id in missing_ids:\n",
    "                    record = {f'feature_{i:02d}': 0.0 for i in range(79)}\n",
    "                    record['symbol_id'] = missing_id\n",
    "                    record['valid'] = 0\n",
    "                    add_df.append(record)\n",
    "                predict_df = pl.concat([predict_df, pl.DataFrame(add_df, schema=predict_df.schema)], how='diagonal')\n",
    "            predict_df = predict_df.sort('symbol_id')\n",
    "            current_stock_ids += new_ids\n",
    "            if len(new_ids) > 0:\n",
    "                print(f'New ids: {new_ids}')\n",
    "                past_responders_pivot = past_responders_pivot.with_columns(\n",
    "                    pl.lit(0.0).cast(pl.Float32).alias(str(i)) for i in new_ids\n",
    "                )\n",
    "                for id_ in new_ids:\n",
    "                    index = adj.shape[0] if id_ >= current_corr_matrix.shape[0] else id_\n",
    "                    current_corr_matrix = np.insert(current_corr_matrix, index, 0, axis=0)\n",
    "                    current_corr_matrix = np.insert(current_corr_matrix, index, 0, axis=1)\n",
    "                adj_matrices = []\n",
    "                for i in range(adjacency_matrices.shape[0]):\n",
    "                    adj = adjacency_matrices[i, :, :]\n",
    "                    for id_ in new_ids:\n",
    "                        index = adj.shape[0] if id_ >= adj.shape[0] else id_\n",
    "                        adj = np.insert(adj, index, 0, axis=0)\n",
    "                        adj = np.insert(adj, index, 0, axis=1)\n",
    "                    adj_matrices.append(adj)\n",
    "                adjacency_matrices = np.stack(adj_matrices)\n",
    "            \n",
    "            x = torch.tensor(predict_df.drop(['symbol_id', 'valid']).to_numpy(), dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            symbols = predict_df.select(['symbol_id']).fill_null(0).fill_nan(0).to_numpy().flatten()\n",
    "            symbols = torch.tensor(symbols, dtype=torch.int).unsqueeze(0).to(device)\n",
    "            adj_matrix = current_corr_matrix.copy()\n",
    "            adj_matrix[missing_ids, :] = 0\n",
    "            adj_matrix[:, missing_ids] = 0\n",
    "            adj_matrix[np.arange(len(current_stock_ids)), np.arange(len(current_stock_ids))] = 0\n",
    "            adj_matrix = (adj_matrix > CORRELATION_THR).astype(np.int32)\n",
    "            adj_matrix = torch.tensor(adj_matrix, dtype=torch.int, device=device).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                preds = inference_model(x, symbols, adj_matrix).cpu().numpy().flatten()\n",
    "            predict_df = predict_df.with_columns(pl.Series(preds).alias('responder_6'))\n",
    "            predictions = test.join(predict_df, on='symbol_id', how='left').select(['row_id', 'responder_6'])\n",
    "        else:\n",
    "            predictions = test.select('row_id', pl.Series(np.zeros(test.shape[0])).alias('responder_6'))\n",
    "        predictions = predictions.with_columns(pl.col('responder_6').cast(pl.Float32))\n",
    "\n",
    "        if isinstance(predictions, pl.DataFrame):\n",
    "            assert predictions.columns == ['row_id', 'responder_6']\n",
    "        elif isinstance(predictions, pd.DataFrame):\n",
    "            assert (predictions.columns == ['row_id', 'responder_6']).all()\n",
    "        else:\n",
    "            raise TypeError('The predict function must return a DataFrame')\n",
    "        assert len(predictions) == len(test)\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:43:26.217383Z",
     "iopub.status.busy": "2025-01-07T16:43:26.217175Z",
     "iopub.status.idle": "2025-01-07T16:44:01.367573Z",
     "shell.execute_reply": "2025-01-07T16:44:01.366876Z",
     "shell.execute_reply.started": "2025-01-07T16:43:26.217366Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "current_day_data : pl.DataFrame | None = None\n",
    "\n",
    "old_dataset = loader.load(start-60, start-1)\\\n",
    "    .fill_nan(None).fill_null(strategy='zero')\\\n",
    "    .sort(['date_id', 'time_id', 'symbol_id']) \\\n",
    "    .select(COLUMNS).collect()\n",
    "\n",
    "last_train_date = start-1\n",
    "new_dataset = old_dataset.filter(pl.col('date_id') > start-30)\n",
    "if OLD_DATA_FRACTION > 0:\n",
    "    old_dataset = old_dataset.filter(pl.col('date_id') <= start-30)\n",
    "else:\n",
    "    old_dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRELATION_THR = 0.1\n",
    "WINDOW_LEN = 2 if not os.getenv('KAGGLE_IS_COMPETITION_RERUN') else 7\n",
    "past_responders_pivot: pl.DataFrame | None = None\n",
    "current_date_id = -1\n",
    "current_stock_ids = list(range(39))\n",
    "num_dates = 0\n",
    "\n",
    "adjacency_matrices = np.load('/home/lorecampa/projects/jane_street_forecasting/dataset/sources/graph_conv_torch/adjacency_matrices.npy')[:last_train_date+1, :, :]\n",
    "current_corr_matrix = np.load('/home/lorecampa/projects/jane_street_forecasting/dataset/sources/graph_conv_torch/correlations.npy')[last_train_date, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-07T16:44:01.800212Z",
     "iopub.status.busy": "2025-01-07T16:44:01.799946Z",
     "iopub.status.idle": "2025-01-07T16:44:01.826177Z",
     "shell.execute_reply": "2025-01-07T16:44:01.825150Z",
     "shell.execute_reply.started": "2025-01-07T16:44:01.800188Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    global BATCH_SIZE, GRADIENT_CLIPPING, N_EPOCHS_PER_TRAIN_MAX, TRAIN_EVERY\n",
    "    global EARLY_STOPPING_DAYS, ES_PATIENCE, OLD_DATA_FRACTION\n",
    "    global MAX_FINE_TUNING_TIME_LIMIT, TIME_LIMIT, FINE_TUNING\n",
    "    global CORRELATION_THR, WINDOW_LEN\n",
    "    global date_idx, new_dataset, old_dataset, current_day_data, last_train_date\n",
    "    global train_dataloader, val_dataloader, train_iterator, val_iterator, adjacency_matrices\n",
    "    global acc_metrics, save_path, start_train, is_training_loop, epoch, best_epoch, best_score\n",
    "    global current_stock_ids, current_date_id, current_corr_matrix, num_dates, past_responders_pivot\n",
    "    global gradient_clipping_decay, gradient_clipping, lr, lr_decay, optimizer\n",
    "\n",
    "    initial_time = time.time()\n",
    "    FINE_TUNING = FINE_TUNING & (initial_time < MAX_FINE_TUNING_TIME_LIMIT)\n",
    "    start_train = start_train if FINE_TUNING else False\n",
    "\n",
    "    if lags is not None:\n",
    "        # print(f\"Date id: {test['date_id'].min()}\")\n",
    "        # new date_id\n",
    "        lags_ = lags.select(\n",
    "            pl.col('date_id').sub(1),\n",
    "            pl.col(['time_id', 'symbol_id']),\n",
    "            pl.col('responder_6_lag_1').alias('responder_6'),\n",
    "        )\n",
    "        if current_day_data is not None:\n",
    "            # print(current_day_data, new_dataset['date_id'].unique().to_list(), old_dataset['date_id'].unique().to_list())\n",
    "            current_day_data = current_day_data.join(lags_, on=['date_id', 'time_id', 'symbol_id'], \n",
    "                                                     how='left').fill_null(0)\n",
    "            current_day_data = current_day_data.select(COLUMNS)\n",
    "            # replacing date id to ensure that adjacency_matrices array is consistent\n",
    "            current_day_data = (\n",
    "                current_day_data\n",
    "                .drop('date_id')\n",
    "                .with_columns(pl.lit(last_train_date + date_idx + 1).cast(pl.Int16).alias('date_id'))\n",
    "                .select(COLUMNS)\n",
    "            )\n",
    "\n",
    "            new_dataset = new_dataset.vstack(current_day_data)\n",
    "            last_adj = current_corr_matrix.copy()\n",
    "            \n",
    "            last_adj[np.arange(len(current_stock_ids)), np.arange(len(current_stock_ids))] = 0\n",
    "            last_adj = (last_adj > CORRELATION_THR).astype(np.int32)[np.newaxis, :, :]\n",
    "            adjacency_matrices = np.concatenate([adjacency_matrices, last_adj], axis=0)\n",
    "            \n",
    "        current_day_data = test\n",
    "\n",
    "        all_combinations = (\n",
    "            lags_.select(['date_id', 'time_id'])\n",
    "            .unique()\n",
    "            .join(pl.DataFrame({'symbol_id': current_stock_ids}, \n",
    "                               schema={'symbol_id': pl.Int8}), how=\"cross\")\n",
    "        )\n",
    "        \n",
    "        pivot_lags = (\n",
    "            all_combinations\n",
    "            .join(lags_, on=['date_id', 'time_id', 'symbol_id'], how=\"left\")\n",
    "            .fill_null(0)\n",
    "            .sort(['date_id', 'time_id', 'symbol_id'])\n",
    "            .pivot(index=['date_id', 'time_id'], values='responder_6', on='symbol_id')\n",
    "            .fill_null(0)\n",
    "        )\n",
    "        \n",
    "        past_responders_pivot = (\n",
    "            pl.concat([past_responders_pivot, pivot_lags], how='diagonal')\n",
    "            .filter(pl.col('date_id') >= current_date_id - WINDOW_LEN - 1)\n",
    "        ) if past_responders_pivot is not None else pivot_lags\n",
    "        \n",
    "        if num_dates >= WINDOW_LEN:\n",
    "            current_corr_matrix = compute_correlation_from_pivot(past_responders_pivot)\n",
    "\n",
    "        if FINE_TUNING and not start_train:\n",
    "            start_train = (date_idx+1) % TRAIN_EVERY == 0\n",
    "            if start_train:\n",
    "                print('Starting new fine tuning')\n",
    "                model.eval()\n",
    "                max_date = new_dataset.select(pl.col('date_id').max()).item()\n",
    "                new_validation_dataset = new_dataset.filter(pl.col('date_id') > max_date - EARLY_STOPPING_DAYS)\n",
    "                new_training_dataset = new_dataset.filter(pl.col('date_id') <= max_date - EARLY_STOPPING_DAYS)\n",
    "                train_days = new_training_dataset['date_id'].unique().sort().to_list()\n",
    "                val_days = new_validation_dataset['date_id'].unique().sort().to_list()\n",
    "                print(f'Training days: {train_days}')\n",
    "                print(f'Validation days: {val_days}')\n",
    "                \n",
    "                if OLD_DATA_FRACTION > 0:\n",
    "                    old_data_len = OLD_DATA_FRACTION * new_training_dataset.shape[0] / (1 - OLD_DATA_FRACTION)\n",
    "                    time_factions = min(1, old_data_len / old_dataset.shape[0])\n",
    "                    old_date_times = old_dataset.select(['date_id', 'time_id']).unique().sample(fraction=time_factions)\n",
    "                                        \n",
    "                    old_training_dataset = old_dataset.join(old_date_times, on=['date_id', 'time_id'], how='inner')\n",
    "                    \n",
    "                    print(f'Old training days: {old_training_dataset[\"date_id\"].unique().to_list()}')\n",
    "                    \n",
    "                    train_dataloader = MultiStockGraphDataset(pl.concat([old_training_dataset, new_training_dataset]), adjacency_matrices.copy(), current_stock_ids)\n",
    "                    val_dataloader = MultiStockGraphDataset(new_validation_dataset, adjacency_matrices.copy(), current_stock_ids)\n",
    "                else:\n",
    "                    train_dataloader = MultiStockGraphDataset(new_training_dataset, adjacency_matrices.copy(), current_stock_ids)\n",
    "                    val_dataloader = MultiStockGraphDataset(new_validation_dataset, adjacency_matrices.copy(), current_stock_ids)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "                train_dataloader = DataLoader(train_dataloader, shuffle=True, batch_size=BATCH_SIZE, num_workers=0)\n",
    "                val_dataloader = DataLoader(val_dataloader, shuffle=False, batch_size=2048, num_workers=0)\n",
    "                val_iterator = iter(val_dataloader)\n",
    "                acc_metrics = dict(ss_res=0.0, ss_tot=0.0)\n",
    "                is_training_loop = False\n",
    "                epoch = -1\n",
    "                best_epoch = -1\n",
    "                best_score = -1e10\n",
    "\n",
    "                if OLD_DATA_FRACTION > 0:\n",
    "                    max_new_date_id = new_training_dataset['date_id'].max()\n",
    "                    old_dataset = old_dataset.vstack(new_training_dataset).filter(\n",
    "                        pl.col('date_id').is_between(max_new_date_id - 30, max_new_date_id)\n",
    "                    )\n",
    "                    \n",
    "                new_dataset = new_validation_dataset\n",
    "                \n",
    "        date_idx += 1\n",
    "    else:\n",
    "        current_day_data = current_day_data.vstack(test)\n",
    "        \n",
    "    if FINE_TUNING:\n",
    "        while start_train and time.time() - initial_time < TIME_LIMIT:\n",
    "            if is_training_loop:\n",
    "                try:\n",
    "                    batch = next(train_iterator)\n",
    "                except StopIteration:\n",
    "                    model.eval()\n",
    "                    val_iterator = iter(val_dataloader)\n",
    "                    acc_metrics = dict(ss_res=0.0, ss_tot=0.0)\n",
    "                    is_training_loop = False\n",
    "                    continue\n",
    "        \n",
    "                x, targets, m, w, s, A = batch\n",
    "                optimizer.zero_grad()\n",
    "                y_out = model.forward(x.to(device), s.to(device), A.to(device)).squeeze()\n",
    "                loss = loss_fn(y_out, targets.to(device), w.to(device))\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)\n",
    "                optimizer.step()\n",
    "                \n",
    "            else:\n",
    "                try:\n",
    "                    batch = next(val_iterator)\n",
    "                except StopIteration:\n",
    "                    score = 1 - acc_metrics['ss_res'] / acc_metrics['ss_tot']\n",
    "                    print(f'Epoch {epoch} completed with score {score}')\n",
    "                    epoch += 1\n",
    "                    if score > best_score:\n",
    "                        torch.save(model.state_dict(), save_path)\n",
    "                        inference_model.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "                        inference_model.to(device)\n",
    "                        inference_model.eval()\n",
    "                        best_epoch = epoch\n",
    "                        best_score = score\n",
    "                    if epoch - best_epoch >= ES_PATIENCE or epoch == N_EPOCHS_PER_TRAIN_MAX:\n",
    "                        print(f'Stopping after {epoch} epochs')\n",
    "                        print(f'Completed Fine Tuning at time {test.select(pl.col(\"time_id\").first()).item()}')\n",
    "                        model.load_state_dict(torch.load(save_path, weights_only=True))\n",
    "                        model.to(device)\n",
    "                        model.eval()\n",
    "                        start_train = False\n",
    "                        lr *= lr_decay\n",
    "                        gradient_clipping *= gradient_clipping_decay\n",
    "                        break\n",
    "                    model.train()\n",
    "                    train_iterator = iter(train_dataloader)\n",
    "                    is_training_loop = True\n",
    "                    continue\n",
    "\n",
    "                x, targets, m, w, s, A = batch\n",
    "                with torch.no_grad():\n",
    "                    y_out = model(x.to(device), s.to(device), A.to(device)).squeeze()\n",
    "                w = w.to(device)\n",
    "                targets = targets.to(device)\n",
    "                acc_metrics['ss_res'] += (w * (y_out - targets) ** 2).sum().cpu()\n",
    "                acc_metrics['ss_tot'] += (w * (targets ** 2)).sum().cpu()\n",
    "\n",
    "    if test.select(pl.col('is_scored').cast(pl.Int8).first()).item() > 0:\n",
    "        test_ = test.fill_nan(None).fill_null(strategy='zero')\n",
    "        predict_df = (\n",
    "            test_.select(['date_id', 'time_id'])\n",
    "                .unique()\n",
    "                .join(pl.DataFrame({'symbol_id': list(range(39))}, \n",
    "                                schema={'symbol_id': pl.Int8}), how=\"cross\")\n",
    "                .join(test_.with_columns(pl.lit(1).alias('mask')), \n",
    "                    on=['date_id', 'time_id', 'symbol_id'], how=\"left\")\n",
    "                .fill_null(0)  # fill all columns with 0 for missing stocks (including the mask)\n",
    "                .sort(['date_id', 'time_id', 'symbol_id'])\n",
    "        )\n",
    "        valid_data = predict_df.select(['mask']).to_numpy().flatten() == 1\n",
    "        x = torch.tensor(predict_df.select([f'feature_{i:02d}' for i in range(79)]).to_numpy().reshape(-1, 39, 79), dtype=torch.float32).to(device)\n",
    "        s = torch.tensor(predict_df.select(['symbol_id']).to_numpy().flatten().reshape(-1, 39).astype(np.int32)).to(device)\n",
    "        # adj = adjacency_matrices[predict_df.select(pl.col('date_id').first()).item()][np.newaxis, :, :]\n",
    "        # adj = torch.tensor(adj, dtype=torch.int, device=device).repeat(x.shape[0], 1, 1)\n",
    "        \n",
    "        adj_matrix = current_corr_matrix.copy()\n",
    "        adj_matrix[np.arange(39), np.arange(39)] = 0\n",
    "        adj_matrix = (adj_matrix > CORRELATION_THR).astype(np.int32)\n",
    "        adj_matrix = torch.tensor(adj_matrix, dtype=torch.int, device=device).unsqueeze(0).repeat(x.shape[0], 1, 1)\n",
    "        with torch.no_grad():\n",
    "            preds = inference_model(x, s, adj_matrix).cpu().numpy().flatten()[valid_data]\n",
    "        predict_df = predict_df.filter(pl.col('mask') == 1).with_columns(pl.Series(preds).alias('responder_6'))\n",
    "        \n",
    "        predictions = test.join(predict_df, on=['time_id', 'symbol_id'], how='left').select(['row_id', 'responder_6'])\n",
    "    else:\n",
    "        predictions = test.select('row_id', pl.Series(np.zeros(test.shape[0])).alias('responder_6'))\n",
    "    predictions = predictions.with_columns(pl.col('responder_6').cast(pl.Float32))\n",
    "\n",
    "    if isinstance(predictions, pl.DataFrame):\n",
    "        assert predictions.columns == ['row_id', 'responder_6']\n",
    "    elif isinstance(predictions, pd.DataFrame):\n",
    "        assert (predictions.columns == ['row_id', 'responder_6']).all()\n",
    "    else:\n",
    "        raise TypeError('The predict function must return a DataFrame')\n",
    "    \n",
    "    assert len(predictions) == len(test)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170/170 [00:49<00:00,  3.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.016260981559753418"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nbs.tabm.predict_tabm import predict_tabm\n",
    "from prj.utils import online_iterator, online_iterator_daily\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "y_hat_iterator = []\n",
    "for i, (test, lags) in enumerate(online_iterator_daily(test_ds, show_progress=True)):\n",
    "    # print(len(test))\n",
    "    res = predict(test, lags)\n",
    "    y_hat_iterator.append(res['responder_6'].to_numpy())\n",
    "    \n",
    "y_hat_iterator = np.concatenate(y_hat_iterator) if len(y_hat_iterator) > 0 else None\n",
    "\n",
    "\n",
    "score = r2_score(y_test, y_hat_iterator, sample_weight=w_test)\n",
    "\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.013081669807434082\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.011371493339538574"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    },
    {
     "datasetId": 6315458,
     "sourceId": 10394435,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 213404970,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 214901608,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 215844687,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 215868163,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

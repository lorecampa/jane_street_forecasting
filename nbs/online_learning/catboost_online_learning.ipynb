{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prj.config import DATA_DIR\n",
    "from prj.data.data_loader import DataConfig, DataLoader\n",
    "import polars as pl\n",
    "from sklearn.metrics import r2_score\n",
    "from catboost import CatBoostRegressor\n",
    "from prj.data.data_loader import PARTITIONS_DATE_INFO\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from prj.utils import online_iterator, online_iterator_daily\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "SEED = 42\n",
    "import lleaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostRegressor at 0x72f9c7947400>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_weighted_loss = False\n",
    "base_params = {\n",
    "    'verbose': 50,\n",
    "    'iterations': 717,\n",
    "    'learning_rate': 0.019678599283449602,\n",
    "    'depth': 8,\n",
    "    'has_time': False,\n",
    "    'bootstrap_type': 'Bernoulli',\n",
    "    'reg_lambda': 0.00924440304487912,\n",
    "    'min_data_in_leaf': 72,\n",
    "    'subsample': 0.63603957073985,\n",
    "    'task_type': 'GPU',\n",
    "}\n",
    "\n",
    "model = CatBoostRegressor()\n",
    "model_file_path = DATA_DIR / 'models' / 'catboost' / 'catboost_model.cbm'\n",
    "model.load_model(model_file_path)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'has_time': False,\n",
       " 'bootstrap_type': 'Bernoulli',\n",
       " 'verbose': 50,\n",
       " 'iterations': 717,\n",
       " 'l2_leaf_reg': 0.009244403045,\n",
       " 'loss_function': 'RMSE',\n",
       " 'subsample': 0.6360395707,\n",
       " 'task_type': 'GPU',\n",
       " 'depth': 8,\n",
       " 'min_data_in_leaf': 72,\n",
       " 'learning_rate': 0.01967859928}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:04<00:00, 17.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 1594-1600\n",
      "Skipping 1595-1600\n",
      "Skipping 1596-1600\n",
      "Skipping 1597-1600\n",
      "Skipping 1598-1600\n",
      "Skipping 1599-1600\n",
      "Skipping 1600-1600\n"
     ]
    }
   ],
   "source": [
    "data_args = {'include_time_id': True, 'include_intrastock_norm_temporal': True}\n",
    "config = DataConfig(**data_args)\n",
    "loader = DataLoader(data_dir=DATA_DIR, config=config)\n",
    "# start_dt, end_dt = 1530, 1698\n",
    "start_dt, end_dt = 1530, 1600\n",
    "# start_dt, end_dt = 1600, 1635\n",
    "test_ds = loader.load(start_dt, end_dt)\n",
    "X_test, y_test, w_test, _ = loader._build_splits(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00936511603357526"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = model.predict(X_test).clip(-5, 5).flatten()\n",
    "offline_score = r2_score(y_test, y_hat, sample_weight=w_test)\n",
    "offline_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n"
     ]
    }
   ],
   "source": [
    "features = loader.features\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_FEATURES = [0, 2, 3, 5, 6, 7, 18, 19, 34, 35, 36, 37, 38, 41, 43, 44, 48, 53, 55, 59, 62, 65, 68, 73, 74, 75, 76, 77, 78]\n",
    "STD_FEATURES = [39, 42, 46, 53, 57, 66]\n",
    "SKEW_FEATURES = [5, 40, 41, 42, 43, 44]\n",
    "ZSCORE_FEATURES = [1, 36, 40, 45, 48, 49, 51, 52, 53, 54, 55, 59, 60]\n",
    "\n",
    "def include_intrastock_norm(df: pl.LazyFrame, responder) -> pl.LazyFrame:\n",
    "    df = df.with_columns(\n",
    "        pl.col([f'feature_{j:02d}' for j in set(MEAN_FEATURES + ZSCORE_FEATURES)]).mean().over(['date_id', 'time_id', f'cluster_label_{responder}']).name.suffix(f'_{responder}_mean'),\n",
    "        pl.col([f'feature_{j:02d}' for j in set(STD_FEATURES + ZSCORE_FEATURES)]).std().over(['date_id', 'time_id', f'cluster_label_{responder}']).name.suffix(f'_{responder}_std'),\n",
    "        pl.col([f'feature_{j:02d}' for j in SKEW_FEATURES]).skew().over(['date_id', 'time_id', f'cluster_label_{responder}']).name.suffix(f'_{responder}_skew'),\n",
    "    ).with_columns(\n",
    "        pl.col(f'feature_{j:02d}').sub(f'feature_{j:02d}_{responder}_mean').truediv(f'feature_{j:02d}_{responder}_std').name.suffix(f'_{responder}_zscore') for j in ZSCORE_FEATURES\n",
    "    ).drop([f'feature_{j:02d}_{responder}_std' for j in ZSCORE_FEATURES if j not in STD_FEATURES] + \\\n",
    "        [f'feature_{j:02d}_{responder}_mean' for j in ZSCORE_FEATURES if j not in MEAN_FEATURES])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DataConfig()\n",
    "loader = DataLoader(data_dir=DATA_DIR, config=config)\n",
    "test_ds = loader.load(start_dt-1, end_dt).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool\n",
    "MAX_ITERATIONS = 1000\n",
    "FINE_TUNING_TIME_LIMIT = 50\n",
    "\n",
    "class CatboostTimeLimitCallback:\n",
    "    def __init__(self, time_limit):\n",
    "        self.time_limit = time_limit\n",
    "        self.start_time = None\n",
    "\n",
    "    def after_iteration(self, info):\n",
    "        if self.start_time is None:\n",
    "            self.start_time = time.time()\n",
    "\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        if elapsed_time > self.time_limit:\n",
    "            print(f\"Stopping training after {elapsed_time:.2f} seconds (time limit reached). Iteration {info.iteration}\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "def build_splits(df: pl.DataFrame, features: list):\n",
    "    X = df.select(features).to_numpy()\n",
    "    y = df['responder_6'].to_numpy().flatten()\n",
    "    w = df['weight'].to_numpy().flatten()\n",
    "    return X, y, w\n",
    "\n",
    "def train_with_es(init_model: CatBoostRegressor, params: dict, train_df: pl.DataFrame, val_df: pl.DataFrame, use_weighted_loss, es_patience, task_type = 'CPU', max_iters = 1000):\n",
    "    start_time = time.time()\n",
    "    _params = params.copy()\n",
    "    _params.pop('iterations', None)\n",
    "    _params.pop('task_type', None)\n",
    "        \n",
    "    X_train, y_train, w_train = build_splits(train_df, features)\n",
    "    train_pool = Pool(data=X_train, label=y_train, weight=w_train if use_weighted_loss else None)\n",
    "    del X_train, y_train, w_train\n",
    "    gc.collect()\n",
    "    \n",
    "    is_early_stopping = val_df is not None and val_df.shape[0] > 0\n",
    "    \n",
    "    if is_early_stopping:\n",
    "        X_val, y_val, w_val = build_splits(val_df, features)\n",
    "        val_pool = Pool(data=X_val, label=y_val, weight=w_val if use_weighted_loss else None)\n",
    "        del X_val, y_val, w_val\n",
    "        gc.collect()\n",
    "\n",
    "    \n",
    "    print(f\"Learning rate: {_params['learning_rate']:e}\")\n",
    "    model = CatBoostRegressor(\n",
    "        iterations=max_iters,\n",
    "        task_type=task_type,\n",
    "        **_params\n",
    "    )\n",
    "        \n",
    "    model.fit(\n",
    "        train_pool,\n",
    "        init_model=init_model,\n",
    "        eval_set=val_pool if is_early_stopping else None,\n",
    "        early_stopping_rounds=es_patience if is_early_stopping else None,\n",
    "        callbacks=[CatboostTimeLimitCallback(50)] if task_type != 'GPU' else None,\n",
    "    )\n",
    "    print(f'Train completed in {((time.time() - start_time)/60):.3f} minutes')\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:01<00:00, 18.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 1523-1529\n",
      "Skipping 1524-1529\n",
      "Skipping 1525-1529\n",
      "Skipping 1526-1529\n",
      "Skipping 1527-1529\n",
      "Skipping 1528-1529\n",
      "Skipping 1529-1529\n"
     ]
    }
   ],
   "source": [
    "responder_replay_buffer_config = DataConfig()\n",
    "responder_replay_buffer_loader = DataLoader(data_dir=DATA_DIR, config=responder_replay_buffer_config)\n",
    "base_responder_replay_buffer = responder_replay_buffer_loader.load(start_dt-1-loader.window_period, start_dt-2)\\\n",
    "    .select('date_id', 'time_id', 'symbol_id', 'responder_6')\\\n",
    "    .with_columns(pl.col('date_id').sub(start_dt))\\\n",
    "    .collect()\n",
    "    \n",
    "\n",
    "TREE_OLD_DATASET_MAX_HISTORY = 30\n",
    "AUX_COLS = ['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']\n",
    "\n",
    "data_config = DataConfig(**data_args)\n",
    "loader = DataLoader(data_dir=DATA_DIR, config=data_config)\n",
    "base_old_dataset = loader.load(start_dt-TREE_OLD_DATASET_MAX_HISTORY, start_dt-1)\\\n",
    "    .select(AUX_COLS + features) \\\n",
    "    .with_columns(pl.col('date_id').sub(start_dt)) \\\n",
    "    .collect()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, w):\n",
    "    y_hat = model.predict(X).clip(-5, 5).flatten()\n",
    "    return r2_score(y, y_hat, sample_weight=w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 27/71 [00:03<00:06,  7.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning model on date 27\n",
      "Old days:  [-30, -29, -28, -27, -26, -25, -24, -23, -22, -21, -20, -19, -18, -17, -16, -15, -14, -13, -12, -11, -10, -9, -8, -7, -6, -5, -4, -3, -2, -1]\n",
      "Train days:  [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "Val days:  []\n",
      "Shapes:  (992200, 139) (0, 139)\n",
      "Learning rate: 8.000000e-06\n",
      "0:\tlearn: 0.7373029\ttotal: 144ms\tremaining: 14.3s\n",
      "50:\tlearn: 0.7372972\ttotal: 7.9s\tremaining: 7.59s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 28/71 [00:22<04:05,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99:\tlearn: 0.7372918\ttotal: 15.4s\tremaining: 0us\n",
      "Train completed in 0.308 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 55/71 [00:26<00:02,  7.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning model on date 55\n",
      "Old days:  [-4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]\n",
      "Train days:  [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54]\n",
      "Val days:  []\n",
      "Shapes:  (1049312, 139) (0, 139)\n",
      "Learning rate: 6.400000e-06\n",
      "0:\tlearn: 0.8111659\ttotal: 186ms\tremaining: 18.4s\n",
      "50:\tlearn: 0.8111332\ttotal: 9.01s\tremaining: 8.66s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 56/71 [00:47<01:36,  6.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99:\tlearn: 0.8111005\ttotal: 17.6s\tremaining: 0us\n",
      "Train completed in 0.348 minutes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:49<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Online score: 0.0093, Offline score: 0.0094 -> Gain: -0.000069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import sum_models\n",
    "from prj.utils import timeout\n",
    "\n",
    "verbose=True\n",
    "\n",
    "class TrainerConfig:\n",
    "    TREE_FINE_TUNING = True\n",
    "    \n",
    "    TREE_REFIT_EVERY = 1000\n",
    "    TREE_TRAIN_EVERY = 28\n",
    "    TREE_OLD_DATA_FRACTION = 0.\n",
    "    TREE_ES_RATIO = 0.15\n",
    "    TREE_ES_PATIENCE = 50\n",
    "    TREE_LR_DECAY = 0.8\n",
    "    TREE_USE_WEIGHTED_LOSS = True\n",
    "    TREE_MAX_FINE_TUNING_TIME_LIMIT = time.time() + 60 * 60 * 8\n",
    "    MAX_HISTORY_DAYS = 30\n",
    "    USE_INTRA_STOCK_NORM = True\n",
    "    USE_TIME_NORM_ID = True\n",
    "    \n",
    "    DEFAULT_MAX_TIME_ID = 967\n",
    "    DEFAULT_CLUSTER = -1\n",
    "    INTRASTOCK_WINDOW_PERIOD = 7\n",
    "    \n",
    "    INITIAL_ONLINE_LR = 1e-5\n",
    "    \n",
    "    \n",
    "class ModelTrainer:\n",
    "    def __init__(self, model, params, old_dataset: pl.DataFrame, responder_replay_buffer: pl.DataFrame):\n",
    "        self.model = model\n",
    "        self.params = params.copy()\n",
    "        \n",
    "        self.config = TrainerConfig()\n",
    "        \n",
    "        \n",
    "        self.params['learning_rate'] = self.config.INITIAL_ONLINE_LR\n",
    "\n",
    "\n",
    "        self.old_dataset = old_dataset\n",
    "        self.new_dataset: pl.DataFrame | None = None\n",
    "        self.current_day_data = None\n",
    "        self.responder_replay_buffer = responder_replay_buffer\n",
    "        self.stock_cluster_mapping = {}\n",
    "        self.stock_max_time_id = {}\n",
    "        self.date_idx = 0\n",
    "        self.corr_responder = 'responder_6'\n",
    "        \n",
    "        self.catboost_models = []\n",
    "\n",
    "\n",
    "    def fine_tune_model(self, date_id: int):\n",
    "        should_retrain =  (self.date_idx + 1) % self.config.TREE_TRAIN_EVERY == 0\n",
    "        should_refit = (self.date_idx + 1) % self.config.TREE_REFIT_EVERY == 0                   \n",
    "        if should_retrain or should_refit:\n",
    "            print(f\"Fine tuning model on date {date_id}\")\n",
    "            if should_retrain:\n",
    "                train_val_days = self.new_dataset['date_id'].unique().sort().to_numpy()     \n",
    "                len_train_val_days = len(train_val_days)     \n",
    "                \n",
    "                \n",
    "                random_split_type = 'None'\n",
    "                \n",
    "                if random_split_type in ['random_days', 'holdout_first', 'holdout_last']:\n",
    "                    if random_split_type == 'random_days':\n",
    "                        train_days, val_days = train_test_split(train_val_days, test_size=self.config.TREE_ES_RATIO, random_state=SEED)\n",
    "                    elif random_split_type == 'holdout_first':\n",
    "                        split_point = max(int(len_train_val_days * self.config.TREE_ES_RATIO), 1)\n",
    "                        val_days = train_val_days[:split_point]\n",
    "                        train_days = train_val_days[split_point:]\n",
    "                    elif random_split_type == 'holdout_last':\n",
    "                        split_point = max(int(len_train_val_days * (self.config.TREE_ES_RATIO)), 1)\n",
    "                        val_days = train_val_days[-split_point:]\n",
    "                        train_days = train_val_days[:-split_point]\n",
    "                        \n",
    "                    val_df = self.new_dataset.filter(pl.col('date_id').is_in(val_days))\n",
    "                    train_df = self.new_dataset.filter(pl.col('date_id').is_in(train_days))\n",
    "                elif random_split_type == 'random_samples':\n",
    "                    np.random.seed(SEED)\n",
    "                    shuffled_indices = np.random.permutation(len(self.new_dataset))\n",
    "                    split_index = int(len(self.new_dataset) * (1 - self.config.TREE_ES_RATIO))\n",
    "                    train_indices = shuffled_indices[:split_index]\n",
    "                    val_indices = shuffled_indices[split_index:]\n",
    "                    \n",
    "                    val_df = self.new_dataset[val_indices]\n",
    "                    train_df = self.new_dataset[train_indices]                \n",
    "                elif random_split_type == 'None':\n",
    "                    train_df = self.new_dataset\n",
    "                    val_df = self.new_dataset.clear()\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown split type: {random_split_type}\")\n",
    "            else:\n",
    "                train_df = self.new_dataset\n",
    "                val_df = self.new_dataset.clear()\n",
    "            \n",
    "\n",
    "            if verbose:\n",
    "                old_days = self.old_dataset['date_id'].unique().sort().to_list()\n",
    "                train_days = train_df['date_id'].unique().sort().to_list()\n",
    "                val_days = val_df['date_id'].unique().sort().to_list()\n",
    "                print('Old days: ', old_days)\n",
    "                print('Train days: ', train_days)\n",
    "                print('Val days: ', val_days)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            if self.config.TREE_OLD_DATA_FRACTION > 0:\n",
    "                unique_train_val_symbols = self.new_dataset['symbol_id'].unique().to_list()\n",
    "                filtered_old_dataset = self.old_dataset.filter(pl.col('symbol_id').is_in(unique_train_val_symbols))\n",
    "                train_df_len = train_df.shape[0]\n",
    "                old_dataset_len = filtered_old_dataset.shape[0]\n",
    "                old_data_len = min(int(self.config.TREE_OLD_DATA_FRACTION * train_df_len / (1 - self.config.TREE_OLD_DATA_FRACTION)), old_dataset_len)\n",
    "                if verbose:\n",
    "                    print(f\"Adding {old_data_len} old data samples to training set, {self.config.TREE_OLD_DATA_FRACTION * 100:.2f}% of the current training set\")\n",
    "                \n",
    "                train_df = filtered_old_dataset\\\n",
    "                    .sample(n=old_data_len)\\\n",
    "                    .vstack(train_df)\n",
    "                    \n",
    "                del filtered_old_dataset\n",
    "                gc.collect()\n",
    "                \n",
    "            if verbose:\n",
    "                print('Shapes: ', train_df.shape, val_df.shape)\n",
    "            \n",
    "            if should_retrain:\n",
    "                self.params['learning_rate'] = max(self.params['learning_rate'] * self.config.TREE_LR_DECAY, 1e-6)\n",
    "                self.model = train_with_es(\n",
    "                    init_model= self.model, \n",
    "                    train_df=train_df,\n",
    "                    val_df=val_df,\n",
    "                    use_weighted_loss=self.config.TREE_USE_WEIGHTED_LOSS,\n",
    "                    es_patience=self.config.TREE_ES_PATIENCE,\n",
    "                    params=self.params,\n",
    "                    max_iters = 100,\n",
    "                    task_type='CPU',\n",
    "                )\n",
    "                \n",
    "                # model = train_with_es(\n",
    "                #     init_model= None,\n",
    "                #     train_df=train_df,\n",
    "                #     val_df=val_df,\n",
    "                #     use_weighted_loss=self.config.TREE_USE_WEIGHTED_LOSS,\n",
    "                #     es_patience=self.config.TREE_ES_PATIENCE,\n",
    "                #     params=self.params,\n",
    "                #     task_type='GPU',\n",
    "                # )\n",
    "                # self.catboost_models.append(model)\n",
    "                \n",
    "                # if val_df.shape[0] > 0:\n",
    "                #     X_val, y_val, w_val = build_splits(val_df, features)\n",
    "                #     catboost_models_scores = [\n",
    "                #         evaluate_model(m, X_val, y_val, w_val) for m in self.catboost_models\n",
    "                #     ]\n",
    "                #     del X_val, y_val, w_val\n",
    "                #     gc.collect()\n",
    "                #     total_score = sum(catboost_models_scores)\n",
    "                #     print('Catboost models scores: ', catboost_models_scores)\n",
    "                #     weights = [score / total_score for score in catboost_models_scores]\n",
    "                # else:\n",
    "                #     weights = [1 / len(self.catboost_models)] * len(self.catboost_models)\n",
    "                \n",
    "                # print('Weights: ', weights)\n",
    "                # self.model = sum_models(self.catboost_models, weights=weights)\n",
    "                                \n",
    "                \n",
    "                \n",
    "                 \n",
    "                            \n",
    "            new_max_old_dataset_date = self.new_dataset['date_id'].max()\n",
    "            self.old_dataset = pl.concat([\n",
    "                self.old_dataset,\n",
    "                self.new_dataset\n",
    "            ]).filter(\n",
    "                pl.col('date_id').is_between(new_max_old_dataset_date-TREE_OLD_DATASET_MAX_HISTORY, new_max_old_dataset_date)\n",
    "            )\n",
    "            self.new_dataset = None\n",
    "            \n",
    "        \n",
    "\n",
    "    def preprocess_time_norm(self, test: pl.DataFrame, lags: pl.DataFrame | None):\n",
    "        if lags is not None:\n",
    "            stock_max_time_id_map = lags.group_by('symbol_id').agg(pl.col('time_id').max())\n",
    "            self.stock_max_time_id = dict(zip(stock_max_time_id_map['symbol_id'], stock_max_time_id_map['time_id']))\n",
    "            self.default_max_time_id = max(list(self.stock_max_time_id.values()))\n",
    "\n",
    "        return test.with_columns(\n",
    "            pl.col('symbol_id')\\\n",
    "                .replace_strict(\n",
    "                    self.stock_max_time_id, default=self.config.DEFAULT_MAX_TIME_ID, return_dtype=pl.Int16\n",
    "                ).alias('max_prev_stock_time_id')\n",
    "            ).with_columns(\n",
    "                pl.col('time_id').truediv('max_prev_stock_time_id').alias('time_id_norm')\n",
    "            ).drop('max_prev_stock_time_id')\n",
    "\n",
    "\n",
    "    def preprocess_intrastock_norm(self, test: pl.DataFrame, lags: pl.DataFrame | None, corr_responder='responder_6'):\n",
    "        if lags is not None:\n",
    "            _lags = lags.select(\n",
    "                pl.col('date_id').sub(1), 'time_id', 'symbol_id',\n",
    "                pl.col(f'{corr_responder}_lag_1').alias(corr_responder)\n",
    "            )\n",
    "            self.responder_replay_buffer = self.responder_replay_buffer.vstack(_lags).filter(\n",
    "                pl.col('date_id').is_between(self.date_idx - self.config.INTRASTOCK_WINDOW_PERIOD, self.date_idx)\n",
    "            )\n",
    "\n",
    "            pivot = self.responder_replay_buffer.filter(pl.col('date_id') < self.date_idx)\\\n",
    "                .pivot(index=['date_id', 'time_id'], values=[corr_responder], separator='_', on='symbol_id')\\\n",
    "                .sort('date_id', 'time_id')\\\n",
    "                .fill_nan(None)\\\n",
    "                .fill_null(strategy='zero')\n",
    "\n",
    "            corr_cols = [col for col in pivot.columns if col not in ['date_id', 'time_id']]\n",
    "            stocks = [int(col) for col in corr_cols]\n",
    "            df_corr_responder = pivot.select(corr_cols).corr()\n",
    "            linked = linkage(df_corr_responder, method='ward')\n",
    "            cluster_labels = fcluster(linked, t=5, criterion='distance')\n",
    "            self.stock_cluster_mapping = dict(zip(stocks, cluster_labels))\n",
    "\n",
    "        return test.with_columns(\n",
    "            pl.col('symbol_id').replace_strict(\n",
    "                self.stock_cluster_mapping, default=self.config.DEFAULT_CLUSTER, return_dtype=pl.Int8\n",
    "            ).alias(f'cluster_label_{corr_responder}')\n",
    "        ).pipe(\n",
    "            include_intrastock_norm,\n",
    "            corr_responder\n",
    "        ).drop(f'cluster_label_{corr_responder}')\n",
    "\n",
    "\n",
    "    def predict(self, test: pl.DataFrame, lags: pl.DataFrame | None):\n",
    "        if self.config.USE_TIME_NORM_ID:\n",
    "            test = self.preprocess_time_norm(test, lags)\n",
    "            \n",
    "        if self.config.USE_INTRA_STOCK_NORM:\n",
    "            test = self.preprocess_intrastock_norm(test, lags)\n",
    "\n",
    "        FINE_TUNING = self.config.TREE_FINE_TUNING and time.time() < self.config.TREE_MAX_FINE_TUNING_TIME_LIMIT\n",
    "        if FINE_TUNING:\n",
    "            if lags is not None:\n",
    "                if self.current_day_data is not None:\n",
    "                    _lags = lags.select(\n",
    "                        pl.col('date_id').sub(1), 'time_id', 'symbol_id',\n",
    "                        pl.col(f'{self.corr_responder}_lag_1').alias(self.corr_responder)\n",
    "                    )\n",
    "                    self.current_day_data = self.current_day_data.join(_lags, on=['date_id', 'time_id', 'symbol_id'], how='left')\\\n",
    "                        .select(AUX_COLS + features)\n",
    "                    \n",
    "                    self.new_dataset = self.current_day_data if self.new_dataset is None else self.new_dataset.vstack(self.current_day_data)\n",
    "                    \n",
    "                    self.current_day_data = None\n",
    "                    \n",
    "            \n",
    "                curr_date_id = test['date_id'].min()\n",
    "                self.fine_tune_model(date_id=curr_date_id)\n",
    "            \n",
    "            self.current_day_data = test if lags is not None else self.current_day_data.vstack(test)\n",
    "\n",
    "        if lags is not None:\n",
    "            self.date_idx += 1\n",
    "        \n",
    "        if test['is_scored'].any():\n",
    "            X = test.select(features).cast(pl.Float32).to_numpy()\n",
    "            y_hat = self.model.predict(X).clip(-5, 5).flatten()\n",
    "        else:\n",
    "            y_hat = np.zeros(test.shape[0])\n",
    "\n",
    "        return test.select('row_id', pl.Series(y_hat).alias(self.corr_responder))\n",
    "\n",
    "\n",
    "model = CatBoostRegressor(**base_params)\n",
    "model.load_model(model_file_path)\n",
    "params = base_params.copy()\n",
    "\n",
    "trainer = ModelTrainer(model, params, base_old_dataset, base_responder_replay_buffer)\n",
    "y_hat_iterator = []\n",
    "\n",
    "for i, (test, lags) in enumerate(online_iterator_daily(test_ds, show_progress=True)):\n",
    "    res = trainer.predict(test, lags)\n",
    "    y_hat_iterator.append(res['responder_6'].to_numpy())\n",
    "\n",
    "y_hat_iterator = np.concatenate(y_hat_iterator)\n",
    "online_score = r2_score(y_true=y_test, y_pred=y_hat_iterator, sample_weight=w_test)\n",
    "gain = online_score - offline_score\n",
    "\n",
    "print(f'Online score: {online_score:.4f}, Offline score: {offline_score:.4f} -> Gain: {gain:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-29 01:39:55.061317: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-12-29 01:39:55.061358: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-12-29 01:39:55.062745: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-29 01:39:55.070076: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-29 01:39:55.870294: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from prj.config import DATA_DIR\n",
    "from prj.data.data_loader import DataConfig, DataLoader\n",
    "import polars as pl\n",
    "\n",
    "data_args = {}\n",
    "config = DataConfig(**data_args)\n",
    "loader = DataLoader(data_dir=DATA_DIR, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_dt, end_dt = 1100, 1110\n",
    "start_val_dt = 1108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 93)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>date_id</th><th>time_id</th><th>symbol_id</th><th>weight</th><th>feature_00</th><th>feature_01</th><th>feature_02</th><th>feature_03</th><th>feature_04</th><th>feature_05</th><th>feature_06</th><th>feature_07</th><th>feature_08</th><th>feature_09</th><th>feature_10</th><th>feature_11</th><th>feature_12</th><th>feature_13</th><th>feature_14</th><th>feature_15</th><th>feature_16</th><th>feature_17</th><th>feature_18</th><th>feature_19</th><th>feature_20</th><th>feature_21</th><th>feature_22</th><th>feature_23</th><th>feature_24</th><th>feature_25</th><th>feature_26</th><th>feature_27</th><th>feature_28</th><th>feature_29</th><th>feature_30</th><th>feature_31</th><th>feature_32</th><th>&hellip;</th><th>feature_52</th><th>feature_53</th><th>feature_54</th><th>feature_55</th><th>feature_56</th><th>feature_57</th><th>feature_58</th><th>feature_59</th><th>feature_60</th><th>feature_61</th><th>feature_62</th><th>feature_63</th><th>feature_64</th><th>feature_65</th><th>feature_66</th><th>feature_67</th><th>feature_68</th><th>feature_69</th><th>feature_70</th><th>feature_71</th><th>feature_72</th><th>feature_73</th><th>feature_74</th><th>feature_75</th><th>feature_76</th><th>feature_77</th><th>feature_78</th><th>responder_0</th><th>responder_1</th><th>responder_2</th><th>responder_3</th><th>responder_4</th><th>responder_5</th><th>responder_6</th><th>responder_7</th><th>responder_8</th><th>partition_id</th></tr><tr><td>i16</td><td>i16</td><td>i8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>i8</td><td>i8</td><td>i16</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>i64</td></tr></thead><tbody><tr><td>1108</td><td>0</td><td>0</td><td>1.599182</td><td>1.25368</td><td>-0.437875</td><td>1.548298</td><td>1.877996</td><td>1.987537</td><td>-1.361107</td><td>0.715413</td><td>-0.534521</td><td>0.302156</td><td>11</td><td>7</td><td>76</td><td>-0.671134</td><td>0.919318</td><td>-0.23278</td><td>null</td><td>0.572328</td><td>null</td><td>-1.704065</td><td>-0.915655</td><td>0.825131</td><td>-0.228274</td><td>0.56992</td><td>1.112303</td><td>-0.933503</td><td>-0.998992</td><td>1.139633</td><td>1.222561</td><td>0.857662</td><td>0.047741</td><td>0.517775</td><td>-0.173189</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>-1.470734</td><td>null</td><td>-0.826701</td><td>0.366417</td><td>null</td><td>0.211672</td><td>-0.013402</td><td>-0.711842</td><td>0.02793</td><td>0.383899</td><td>0.319894</td><td>null</td><td>null</td><td>-0.529676</td><td>0.347533</td><td>-0.305567</td><td>-1.174261</td><td>2.219099</td><td>-0.190434</td><td>null</td><td>null</td><td>-0.239844</td><td>-0.167164</td><td>-0.151576</td><td>-0.35227</td><td>-0.664202</td><td>-0.006456</td><td>0.213953</td><td>-0.144088</td><td>-0.809349</td><td>-0.137964</td><td>0.297297</td><td>-1.567391</td><td>-0.518465</td><td>6</td></tr><tr><td>1108</td><td>0</td><td>1</td><td>2.844544</td><td>2.107626</td><td>0.634461</td><td>0.900853</td><td>2.303407</td><td>2.171719</td><td>-0.871822</td><td>0.433323</td><td>-0.304671</td><td>0.240534</td><td>11</td><td>7</td><td>76</td><td>-1.075586</td><td>1.673318</td><td>-0.319468</td><td>null</td><td>-0.056212</td><td>null</td><td>-1.617553</td><td>-0.928436</td><td>0.130291</td><td>0.006819</td><td>2.349689</td><td>1.724608</td><td>-0.875158</td><td>-0.176288</td><td>0.51666</td><td>1.788792</td><td>1.231096</td><td>-0.34207</td><td>-0.123342</td><td>0.006035</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>-1.178793</td><td>null</td><td>-1.270172</td><td>1.390535</td><td>null</td><td>1.001953</td><td>0.182568</td><td>-0.711842</td><td>-0.321874</td><td>-0.164353</td><td>-0.228116</td><td>-1.584387</td><td>-1.48613</td><td>-0.856901</td><td>0.728709</td><td>-0.29794</td><td>-0.725641</td><td>2.133685</td><td>-0.09719</td><td>null</td><td>null</td><td>-0.20097</td><td>-0.245658</td><td>-0.328485</td><td>-0.292093</td><td>-0.291818</td><td>-0.210165</td><td>-0.199149</td><td>-0.598838</td><td>-1.154606</td><td>0.206256</td><td>-0.519193</td><td>-1.083733</td><td>0.715887</td><td>6</td></tr><tr><td>1108</td><td>0</td><td>2</td><td>1.778899</td><td>1.098838</td><td>0.408612</td><td>1.382891</td><td>1.297657</td><td>2.518946</td><td>-0.963115</td><td>0.691736</td><td>-0.334735</td><td>0.253715</td><td>81</td><td>2</td><td>59</td><td>-0.854383</td><td>-0.100864</td><td>-0.812462</td><td>null</td><td>-0.172732</td><td>null</td><td>-2.131718</td><td>-1.850823</td><td>-0.939381</td><td>-0.251988</td><td>0.590109</td><td>0.110248</td><td>-1.154203</td><td>-0.847603</td><td>0.848478</td><td>1.35906</td><td>1.375511</td><td>-0.429401</td><td>-0.455752</td><td>-0.250771</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>-2.786971</td><td>null</td><td>-1.279411</td><td>-0.033275</td><td>null</td><td>-0.852217</td><td>-0.420891</td><td>-0.711842</td><td>-0.178966</td><td>-0.10043</td><td>-0.106295</td><td>-2.168841</td><td>-1.996272</td><td>-0.932895</td><td>-0.09625</td><td>-0.554614</td><td>-1.194544</td><td>-0.077133</td><td>-0.452011</td><td>null</td><td>null</td><td>4.90193</td><td>8.509361</td><td>13.77626</td><td>16.06304</td><td>-0.121137</td><td>-0.122312</td><td>-0.135348</td><td>-1.413297</td><td>-0.520816</td><td>0.370638</td><td>-2.777546</td><td>-0.391677</td><td>0.677883</td><td>6</td></tr><tr><td>1108</td><td>0</td><td>3</td><td>1.230088</td><td>1.073081</td><td>0.116436</td><td>1.593973</td><td>1.207443</td><td>2.219879</td><td>-0.576148</td><td>0.509578</td><td>-0.212684</td><td>0.121532</td><td>4</td><td>3</td><td>11</td><td>-0.330236</td><td>7.445848</td><td>1.327974</td><td>null</td><td>-0.188279</td><td>null</td><td>-1.233456</td><td>-2.028468</td><td>-0.194337</td><td>0.049778</td><td>0.009243</td><td>-0.19984</td><td>0.888055</td><td>1.039983</td><td>-0.3943</td><td>-0.463027</td><td>-0.953243</td><td>-0.665958</td><td>-0.658918</td><td>0.047487</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>-1.236717</td><td>null</td><td>-0.556</td><td>1.513677</td><td>null</td><td>3.488591</td><td>1.132235</td><td>-0.711842</td><td>-0.252953</td><td>-0.270835</td><td>-0.271094</td><td>-2.050989</td><td>-1.501235</td><td>-0.628259</td><td>1.939634</td><td>0.022731</td><td>-0.300654</td><td>8.424384</td><td>1.322707</td><td>null</td><td>null</td><td>0.499284</td><td>0.500686</td><td>-0.056693</td><td>-0.037714</td><td>0.666928</td><td>0.058977</td><td>0.312081</td><td>-2.393661</td><td>-0.226595</td><td>-1.017476</td><td>-3.332203</td><td>-0.305786</td><td>-1.549229</td><td>6</td></tr><tr><td>1108</td><td>0</td><td>4</td><td>1.57557</td><td>1.393097</td><td>0.683312</td><td>1.041991</td><td>1.166212</td><td>2.003079</td><td>-0.410651</td><td>0.394495</td><td>-0.23376</td><td>0.115619</td><td>15</td><td>1</td><td>9</td><td>-0.722064</td><td>-0.091613</td><td>-0.638488</td><td>null</td><td>-0.020833</td><td>null</td><td>-1.600799</td><td>-1.023754</td><td>1.333991</td><td>0.038441</td><td>-0.04504</td><td>-0.510163</td><td>0.520042</td><td>1.171758</td><td>-1.428435</td><td>-1.115971</td><td>-0.219436</td><td>-0.552869</td><td>-0.737578</td><td>0.046018</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>-1.363041</td><td>null</td><td>-1.086848</td><td>1.488567</td><td>null</td><td>-1.27407</td><td>-0.939332</td><td>-0.711842</td><td>-0.226862</td><td>0.043165</td><td>-0.108489</td><td>-1.151456</td><td>-2.267298</td><td>-0.616098</td><td>-0.022931</td><td>-0.532399</td><td>-0.905603</td><td>-0.133107</td><td>-0.636313</td><td>null</td><td>null</td><td>2.806377</td><td>2.141493</td><td>0.572724</td><td>0.296508</td><td>0.127061</td><td>-0.0509</td><td>1.144851</td><td>-2.50963</td><td>-1.570697</td><td>-1.69568</td><td>-2.117235</td><td>-1.251168</td><td>-2.565703</td><td>6</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 93)\n",
       "┌─────────┬─────────┬───────────┬──────────┬───┬────────────┬────────────┬────────────┬────────────┐\n",
       "│ date_id ┆ time_id ┆ symbol_id ┆ weight   ┆ … ┆ responder_ ┆ responder_ ┆ responder_ ┆ partition_ │\n",
       "│ ---     ┆ ---     ┆ ---       ┆ ---      ┆   ┆ 6          ┆ 7          ┆ 8          ┆ id         │\n",
       "│ i16     ┆ i16     ┆ i8        ┆ f32      ┆   ┆ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
       "│         ┆         ┆           ┆          ┆   ┆ f32        ┆ f32        ┆ f32        ┆ i64        │\n",
       "╞═════════╪═════════╪═══════════╪══════════╪═══╪════════════╪════════════╪════════════╪════════════╡\n",
       "│ 1108    ┆ 0       ┆ 0         ┆ 1.599182 ┆ … ┆ 0.297297   ┆ -1.567391  ┆ -0.518465  ┆ 6          │\n",
       "│ 1108    ┆ 0       ┆ 1         ┆ 2.844544 ┆ … ┆ -0.519193  ┆ -1.083733  ┆ 0.715887   ┆ 6          │\n",
       "│ 1108    ┆ 0       ┆ 2         ┆ 1.778899 ┆ … ┆ -2.777546  ┆ -0.391677  ┆ 0.677883   ┆ 6          │\n",
       "│ 1108    ┆ 0       ┆ 3         ┆ 1.230088 ┆ … ┆ -3.332203  ┆ -0.305786  ┆ -1.549229  ┆ 6          │\n",
       "│ 1108    ┆ 0       ┆ 4         ┆ 1.57557  ┆ … ┆ -2.117235  ┆ -1.251168  ┆ -2.565703  ┆ 6          │\n",
       "└─────────┴─────────┴───────────┴──────────┴───┴────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_ds = loader.load(start_dt, end_dt)\n",
    "\n",
    "val_ds = complete_ds.filter(pl.col('date_id').ge(start_val_dt))\n",
    "val_ds.collect().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Optional, Tuple, Dict, Any\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import pytorch_lightning as L\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "\n",
    "import kaggle_evaluation.jane_street_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_r2_score(preds, targets, weights):\n",
    "    ss_res = (weights * (targets - preds) ** 2).sum()\n",
    "    ss_tot = (weights * (targets ** 2)).sum()\n",
    "    return 1 - ss_res / ss_tot if ss_tot > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedMSELoss, self).__init__()\n",
    "    \n",
    "    def forward(self, predictions: Tensor, targets: Tensor, weights: Tensor) -> Tensor:\n",
    "        squared_diff = (predictions - targets) ** 2\n",
    "        weighted_squared_diff = weights * squared_diff\n",
    "        return weighted_squared_diff.sum() / weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNNModel(nn.Module):\n",
    "    def __init__(self, input_features, hidden_dims=[], dropout_rate=0.1, output_dim=1, \n",
    "                 use_tanh=False, final_mult=1.0):\n",
    "        super(SimpleNNModel, self).__init__()\n",
    "        self.final_mult = final_mult\n",
    "        self.use_tanh = use_tanh\n",
    "        \n",
    "        layers = [nn.BatchNorm1d(input_features), nn.Dropout(dropout_rate)]\n",
    "        in_features = input_features\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_features, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.SiLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            in_features = hidden_dim\n",
    "            \n",
    "        layers.append(nn.Linear(in_features, output_dim))\n",
    "        if self.use_tanh:\n",
    "            layers.append(nn.Tanh())\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.final_mult * self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaneStreetBaseModel(L.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 model: nn.Module,\n",
    "                 losses: List[nn.Module] | nn.Module, \n",
    "                 loss_weights: List[float], \n",
    "                 l1_lambda: float = 1e-4,\n",
    "                 l2_lambda: float = 1e-4,\n",
    "                 optimizer: str = 'Adam',\n",
    "                 optimizer_cfg: Dict[str, Any] = dict(),\n",
    "                 scheduler: str = None,\n",
    "                 scheduler_cfg: Dict[str, Any] = dict()):\n",
    "        super(JaneStreetBaseModel, self).__init__()   \n",
    "        assert isinstance(losses, nn.Module) or len(losses) == len(loss_weights), 'Each loss must have a weight'\n",
    "        assert len(loss_weights) == 0 or min(loss_weights) > 0, 'Losses must have positive weights'\n",
    "        self.model = model\n",
    "        losses = [losses] if isinstance(losses, nn.Module) else losses\n",
    "        self.losses = nn.ModuleList(losses) \n",
    "        self.loss_weights = [1.0] if isinstance(losses, nn.Module) else loss_weights\n",
    "        self.l1_lambda = l1_lambda\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.optimizer_name = optimizer\n",
    "        self.optimizer_cfg = optimizer_cfg\n",
    "        self.scheduler_name = scheduler\n",
    "        self.scheduler_cfg = scheduler_cfg\n",
    "        self.acc_metrics = dict(ss_res=0.0, ss_tot=0.0, abs_err_sum=0.0, weights_sum=0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, weights = batch\n",
    "        y_hat = self.forward(x).squeeze()\n",
    "        loss = self._compute_loss(y_hat, y, weights)\n",
    "        reg_loss, l1_loss, l2_loss = self._regularization_loss()\n",
    "        metrics = dict()\n",
    "        with torch.no_grad():\n",
    "            metrics['train_wmse'] = (weights * (y_hat - y) ** 2).sum() / weights.sum()\n",
    "            metrics['train_wmae'] = (weights * (y_hat - y).abs()).sum() / weights.sum()\n",
    "            metrics['train_wr2'] = weighted_r2_score(y_hat, y, weights)\n",
    "        metrics['train_loss'] = loss\n",
    "        if self.l1_lambda > 0:\n",
    "            metrics['train_l1_reg'] = l1_loss\n",
    "        if self.l2_lambda > 0:\n",
    "            metrics['train_l2_reg'] = l2_loss\n",
    "        self.log_dict(metrics, on_step=True, on_epoch=True, prog_bar=True, logger=True, batch_size=x.size(0))\n",
    "        return loss + reg_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, weights = batch\n",
    "        y_hat = self.forward(x).squeeze()\n",
    "        loss = self._compute_loss(y_hat, y, weights)\n",
    "        ss_res_step = (weights * (y_hat - y) ** 2).sum()\n",
    "        ss_tot_step = (weights * (y ** 2)).sum()\n",
    "        abs_err_step = (weights * (y_hat - y).abs()).sum()\n",
    "        weights_sum_step = weights.sum()\n",
    "        self.acc_metrics['ss_res'] += ss_res_step\n",
    "        self.acc_metrics['ss_tot'] += ss_tot_step\n",
    "        self.acc_metrics['abs_err_sum'] += abs_err_step\n",
    "        self.acc_metrics['weights_sum'] += weights_sum_step\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, batch_size=x.size(0))\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        metrics = dict()\n",
    "        metrics['val_wmse'] = self.acc_metrics['ss_res'] / self.acc_metrics['weights_sum']\n",
    "        metrics['val_wmae'] = self.acc_metrics['abs_err_sum'] / self.acc_metrics['weights_sum']\n",
    "        metrics['val_wr2'] = 1 - self.acc_metrics['ss_res'] / self.acc_metrics['ss_tot']\n",
    "        self.acc_metrics = dict(ss_res=0.0, ss_tot=0.0, abs_err_sum=0.0, weights_sum=0.0)\n",
    "        self.log_dict(metrics, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def _compute_loss(self, preds, targets, weights):\n",
    "        loss = 0\n",
    "        for i in range(len(self.losses)):\n",
    "            loss += self.losses[i](preds, targets, weights=weights) * self.loss_weights[i]\n",
    "        return loss\n",
    "    \n",
    "    def _regularization_loss(self):\n",
    "        reg_loss = 0\n",
    "        l1_loss = 0\n",
    "        l2_loss = 0\n",
    "        \n",
    "        if self.l1_lambda > 0:\n",
    "            l1_loss = sum(p.abs().sum() for p in self.parameters())\n",
    "            reg_loss += l1_loss * self.l1_lambda\n",
    "            \n",
    "        if self.l2_lambda > 0:\n",
    "            l2_loss = sum(p.pow(2).sum() for p in self.parameters())\n",
    "            reg_loss += l2_loss * self.l2_lambda\n",
    "            \n",
    "        return reg_loss, l1_loss, l2_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = getattr(torch.optim, self.optimizer_name)(self.parameters(), **self.optimizer_cfg)\n",
    "        if self.scheduler_name is None:\n",
    "            return optimizer\n",
    "        scheduler = getattr(torch.optim.lr_scheduler, self.scheduler_name)(optimizer, **self.scheduler_cfg)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val_wr2',\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lorecampa/projects/jane_street_forecasting/.venv/lib/python3.10/site-packages/pytorch_lightning/utilities/migration/utils.py:56: The loaded checkpoint was produced with Lightning v2.5.0.post0, which is newer than your current Lightning version: v2.4.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "JaneStreetBaseModel(\n",
       "  (model): SimpleNNModel(\n",
       "    (model): Sequential(\n",
       "      (0): BatchNorm1d(79, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (1): Dropout(p=0.3, inplace=False)\n",
       "      (2): Linear(in_features=79, out_features=256, bias=True)\n",
       "      (3): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (4): SiLU()\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "      (6): Linear(in_features=256, out_features=1, bias=True)\n",
       "      (7): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (losses): ModuleList(\n",
       "    (0): WeightedMSELoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_base_model = SimpleNNModel(79, hidden_dims=[256], dropout_rate=0.3, final_mult=5.0, use_tanh=True)\n",
    "training_model = JaneStreetBaseModel.load_from_checkpoint(\n",
    "    \"/home/lorecampa/projects/jane_street_forecasting/experiments/saved/nn/model1/baseline_all.ckpt\", \n",
    "    model=training_base_model,\n",
    "    losses=[WeightedMSELoss()],\n",
    "    loss_weights=[1]\n",
    ")\n",
    "\n",
    "inference_base_model = SimpleNNModel(79, hidden_dims=[256], dropout_rate=0.3, final_mult=5.0, use_tanh=True)\n",
    "inference_model = JaneStreetBaseModel.load_from_checkpoint(\n",
    "    \"/home/lorecampa/projects/jane_street_forecasting/experiments/saved/nn/model1/baseline_all.ckpt\", \n",
    "    model=inference_base_model,\n",
    "    losses=[WeightedMSELoss()],\n",
    "    loss_weights=[1]\n",
    ")\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     training_model = training_model.cuda()\n",
    "#     inference_model = inference_model.cuda()\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    training_model.parameters(), \n",
    "    lr=1e-6, \n",
    "    weight_decay=5e-4, \n",
    "    amsgrad=False, \n",
    "    betas=(0.9, 0.99))\n",
    "training_model.train()\n",
    "inference_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in training_model.modules():\n",
    "    # freeze batch normalization parameters\n",
    "    if isinstance(module, nn.BatchNorm2d):\n",
    "        if hasattr(module, 'weight'):\n",
    "            module.weight.requires_grad_(False)\n",
    "        if hasattr(module, 'bias'):\n",
    "            module.bias.requires_grad_(False)\n",
    "        module.eval() # do not change the running mean and var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_teacher(student_model, teacher_model, alpha=0.99):\n",
    "    for teacher_param, student_param in zip(teacher_model.parameters(), student_model.parameters()):\n",
    "        # exponential moving average\n",
    "        teacher_param.data = alpha * teacher_param.data + (1.0 - alpha) * student_param.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JaneStreetBaseDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset: pl.DataFrame):\n",
    "        super(JaneStreetBaseDataset, self).__init__()   \n",
    "        self.dataset = dataset\n",
    "        feature_cols = [f'feature_{i:02d}' for i in range(79)]\n",
    "        self.X = torch.FloatTensor(self.dataset.select(feature_cols).to_numpy().astype(np.float32))\n",
    "        self.y = torch.FloatTensor(self.dataset.select(['responder_6']).to_numpy().flatten().astype(np.float32))\n",
    "        self.weights = torch.FloatTensor(self.dataset.select(['weight']).to_numpy().flatten().astype(np.float32))        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):       \n",
    "        return self.X[idx], self.y[idx], self.weights[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "FINE_TUNING = True\n",
    "BATCH_SIZE = 32768\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    TRAIN_EVERY = 25 # start a new fine tuning every 14 days\n",
    "    N_EPOCHS_PER_TRAIN = 2\n",
    "    DATASET_MAX_SIZE = int(1e7)\n",
    "else:\n",
    "    TRAIN_EVERY = 2 # start a new fine tuning every 2 days\n",
    "    N_EPOCHS_PER_TRAIN = 2\n",
    "    DATASET_MAX_SIZE = int(1e6)\n",
    "    \n",
    "TIME_LIMIT = 30\n",
    "FEATURE_COLS = [f'feature_{i:02d}' for i in range(79)]\n",
    "MAX_FINE_TUNING_TIME_LIMIT = time.time() + 60 * 60 * 8 # after 8 hours, stop all the online learning\n",
    "CLIP_NORM_VALUE = 5\n",
    "dataset : pl.DataFrame | None = None\n",
    "current_day_data : pl.DataFrame | None = None\n",
    "date_idx = 0\n",
    "start_train = False\n",
    "train_dataloader = None\n",
    "train_iterator = None\n",
    "num_epochs_done = 0\n",
    "\n",
    "COLUMNS = FEATURE_COLS + ['date_id', 'time_id', 'symbol_id', 'weight', 'responder_6']\n",
    "dataset = pl.scan_parquet('/home/lorecampa/projects/jane_street_forecasting/dataset/train.parquet/partition_id=8/part-0.parquet')\n",
    "dataset = dataset.select(COLUMNS) \\\n",
    "    .sort(['date_id', 'time_id', 'symbol_id']) \\\n",
    "    .with_columns(pl.col(FEATURE_COLS).fill_null(strategy='zero')) \\\n",
    "    .collect()[-DATASET_MAX_SIZE:, :]\n",
    "\n",
    "\n",
    "def predict(test: pl.DataFrame, lags: pl.DataFrame | None) -> pl.DataFrame | pd.DataFrame:\n",
    "    global FINE_TUNING, BATCH_SIZE, TRAIN_EVERY, DATASET_MAX_SIZE, N_EPOCHS_PER_TRAIN, TIME_LIMIT\n",
    "    global date_idx, dataset, current_day_data, start_train, train_dataloader, num_epochs_done, train_iterator\n",
    "\n",
    "    initial_time = time.time()\n",
    "    FINE_TUNING = initial_time < MAX_FINE_TUNING_TIME_LIMIT\n",
    "    if FINE_TUNING:\n",
    "        if lags is not None:\n",
    "            print('New day')\n",
    "            lags_ = lags.select(\n",
    "                pl.col('date_id').sub(1),\n",
    "                pl.col(['time_id', 'symbol_id']),\n",
    "                pl.col('responder_6_lag_1').alias('responder_6'),\n",
    "            )\n",
    "            if current_day_data is not None:\n",
    "                current_day_data = current_day_data.join(lags_, on=['date_id', 'time_id', 'symbol_id'], \n",
    "                                                         how='left').fill_nan(0).fill_null(0)\n",
    "                current_day_data = current_day_data.select(COLUMNS)\n",
    "                dataset = dataset.vstack(current_day_data)\n",
    "                dataset = dataset[-DATASET_MAX_SIZE:]\n",
    "            # initialize current daily data\n",
    "            current_day_data = test\n",
    "            if not start_train:\n",
    "                start_train = (date_idx + 1) % TRAIN_EVERY == 0\n",
    "                if start_train:\n",
    "                    print('Starting new fine tuning')\n",
    "                    train_dataloader = JaneStreetBaseDataset(dataset)\n",
    "                    train_dataloader = DataLoader(train_dataloader, shuffle=True, batch_size=BATCH_SIZE, num_workers=0)\n",
    "                    train_iterator = iter(train_dataloader)\n",
    "                    num_epochs_done = 0\n",
    "            date_idx += 1\n",
    "        else:\n",
    "            # still the same date_id, no lags provided -> no data to add to replay buffer\n",
    "            current_day_data = current_day_data.vstack(test)\n",
    "\n",
    "        while start_train and time.time() - initial_time < TIME_LIMIT: \n",
    "            try:\n",
    "                batch = next(train_iterator)\n",
    "            except StopIteration:\n",
    "                if num_epochs_done == N_EPOCHS_PER_TRAIN:\n",
    "                    print(f'Completed Fine Tuning at time {test.select(pl.col(\"time_id\").first()).item()}')\n",
    "                    start_train = False\n",
    "                    for training_param, inference_param in zip(training_model.parameters(), inference_model.parameters()):\n",
    "                        inference_param.data = training_param.data\n",
    "                    inference_model.eval()\n",
    "                    break\n",
    "                else:\n",
    "                    num_epochs_done += 1\n",
    "                    train_iterator = iter(train_dataloader)\n",
    "                    batch = next(train_iterator)\n",
    "\n",
    "            x, y, w = batch\n",
    "            optimizer.zero_grad()\n",
    "            y_out = training_model.forward(x.to(training_model.device)).squeeze()\n",
    "            loss = training_model._compute_loss(y_out, y.to(training_model.device), w.to(training_model.device))\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(training_model.parameters(), CLIP_NORM_VALUE)\n",
    "            optimizer.step()\n",
    "\n",
    "    if test.select(pl.col('is_scored').cast(pl.Int8).first()).item() > 0:\n",
    "        predict_df = test.select([f'feature_{i:02d}' for i in range(79)]).fill_null(0).fill_nan(0)\n",
    "        x = torch.tensor(predict_df.to_numpy(), dtype=torch.float32).to(inference_model.device)\n",
    "        with torch.no_grad():\n",
    "            preds = inference_model(x).cpu().numpy().flatten()\n",
    "    else:\n",
    "        preds = np.zeros(test.shape[0])\n",
    "    predictions = test.select('row_id', pl.Series(preds).alias('responder_6'))\n",
    "\n",
    "    if isinstance(predictions, pl.DataFrame):\n",
    "        assert predictions.columns == ['row_id', 'responder_6']\n",
    "    elif isinstance(predictions, pd.DataFrame):\n",
    "        assert (predictions.columns == ['row_id', 'responder_6']).all()\n",
    "    else:\n",
    "        raise TypeError('The predict function must return a DataFrame')\n",
    "    assert len(predictions) == len(test)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (6_274_576, 93)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_id</th><th>date_id</th><th>time_id</th><th>symbol_id</th><th>weight</th><th>feature_00</th><th>feature_01</th><th>feature_02</th><th>feature_03</th><th>feature_04</th><th>feature_05</th><th>feature_06</th><th>feature_07</th><th>feature_08</th><th>feature_09</th><th>feature_10</th><th>feature_11</th><th>feature_12</th><th>feature_13</th><th>feature_14</th><th>feature_15</th><th>feature_16</th><th>feature_17</th><th>feature_18</th><th>feature_19</th><th>feature_20</th><th>feature_21</th><th>feature_22</th><th>feature_23</th><th>feature_24</th><th>feature_25</th><th>feature_26</th><th>feature_27</th><th>feature_28</th><th>feature_29</th><th>feature_30</th><th>feature_31</th><th>&hellip;</th><th>feature_51</th><th>feature_52</th><th>feature_53</th><th>feature_54</th><th>feature_55</th><th>feature_56</th><th>feature_57</th><th>feature_58</th><th>feature_59</th><th>feature_60</th><th>feature_61</th><th>feature_62</th><th>feature_63</th><th>feature_64</th><th>feature_65</th><th>feature_66</th><th>feature_67</th><th>feature_68</th><th>feature_69</th><th>feature_70</th><th>feature_71</th><th>feature_72</th><th>feature_73</th><th>feature_74</th><th>feature_75</th><th>feature_76</th><th>feature_77</th><th>feature_78</th><th>responder_0</th><th>responder_1</th><th>responder_2</th><th>responder_3</th><th>responder_4</th><th>responder_5</th><th>responder_6</th><th>responder_7</th><th>responder_8</th></tr><tr><td>u32</td><td>i16</td><td>i16</td><td>i8</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>i8</td><td>i8</td><td>i16</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>&hellip;</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td><td>f32</td></tr></thead><tbody><tr><td>0</td><td>1530</td><td>0</td><td>0</td><td>3.084694</td><td>1.153571</td><td>1.563784</td><td>0.697396</td><td>0.756759</td><td>2.580965</td><td>0.171311</td><td>1.126353</td><td>0.536153</td><td>0.05715</td><td>11</td><td>7</td><td>76</td><td>-0.656288</td><td>2.110188</td><td>0.145784</td><td>null</td><td>-0.203291</td><td>null</td><td>-1.238222</td><td>-2.294707</td><td>-0.06356</td><td>-0.148218</td><td>1.721362</td><td>0.64558</td><td>1.477857</td><td>0.528492</td><td>1.153077</td><td>0.466157</td><td>0.145568</td><td>-0.546845</td><td>-0.694435</td><td>-0.163897</td><td>&hellip;</td><td>0.680807</td><td>null</td><td>null</td><td>-2.786826</td><td>null</td><td>-1.2279</td><td>0.044606</td><td>null</td><td>-2.540213</td><td>-2.19028</td><td>0.385893</td><td>-0.460265</td><td>-0.415684</td><td>-0.45772</td><td>-1.333965</td><td>-2.23413</td><td>-0.352034</td><td>3.125156</td><td>0.493488</td><td>-0.9591</td><td>1.284456</td><td>-0.275493</td><td>null</td><td>null</td><td>4.188457</td><td>3.666236</td><td>0.848177</td><td>0.999516</td><td>0.417462</td><td>0.323897</td><td>0.601499</td><td>2.074103</td><td>0.746552</td><td>0.552013</td><td>3.071231</td><td>0.914794</td><td>0.997124</td></tr><tr><td>1</td><td>1530</td><td>0</td><td>1</td><td>2.232906</td><td>0.553354</td><td>1.730064</td><td>0.990195</td><td>0.61149</td><td>2.023031</td><td>0.319015</td><td>1.183371</td><td>0.562853</td><td>0.057789</td><td>11</td><td>7</td><td>76</td><td>-1.063518</td><td>1.037634</td><td>-0.255358</td><td>null</td><td>-0.318528</td><td>null</td><td>-1.46613</td><td>-2.160217</td><td>0.009386</td><td>0.042186</td><td>0.319811</td><td>0.14307</td><td>1.866907</td><td>1.238242</td><td>-1.986826</td><td>-0.476918</td><td>0.408439</td><td>-0.689795</td><td>-0.619278</td><td>0.081413</td><td>&hellip;</td><td>-0.00268</td><td>null</td><td>null</td><td>-1.736226</td><td>null</td><td>-2.354893</td><td>1.309985</td><td>null</td><td>-2.429267</td><td>-1.26697</td><td>0.385893</td><td>-0.24877</td><td>-0.286104</td><td>-0.455154</td><td>-1.797363</td><td>-2.535985</td><td>-0.734866</td><td>1.533782</td><td>0.033801</td><td>-0.960126</td><td>0.306505</td><td>-0.522036</td><td>null</td><td>null</td><td>1.138142</td><td>1.579439</td><td>0.179564</td><td>0.160609</td><td>-0.318671</td><td>-0.399384</td><td>-0.635306</td><td>2.092151</td><td>0.342582</td><td>0.757289</td><td>1.979042</td><td>0.967537</td><td>1.219739</td></tr><tr><td>2</td><td>1530</td><td>0</td><td>2</td><td>2.404948</td><td>1.532503</td><td>2.095852</td><td>0.919688</td><td>0.583715</td><td>2.330047</td><td>0.337096</td><td>1.262236</td><td>0.49605</td><td>0.073556</td><td>81</td><td>2</td><td>59</td><td>-1.001967</td><td>1.10577</td><td>-0.304426</td><td>null</td><td>-0.531873</td><td>null</td><td>-1.301579</td><td>-1.615271</td><td>0.454406</td><td>-0.188808</td><td>0.01512</td><td>-0.159487</td><td>1.379064</td><td>0.604568</td><td>0.736194</td><td>0.522007</td><td>-0.183058</td><td>-0.632819</td><td>-0.839542</td><td>-0.20955</td><td>&hellip;</td><td>0.793936</td><td>null</td><td>null</td><td>-1.191118</td><td>null</td><td>-2.190607</td><td>1.381697</td><td>null</td><td>-1.829545</td><td>-0.867858</td><td>0.385893</td><td>-0.295958</td><td>-0.386221</td><td>-0.345102</td><td>-1.598371</td><td>-2.111468</td><td>-0.780465</td><td>0.848857</td><td>-0.152994</td><td>-1.219395</td><td>0.359229</td><td>-0.636138</td><td>null</td><td>null</td><td>0.445388</td><td>0.300118</td><td>-0.043114</td><td>-0.065761</td><td>0.200878</td><td>-0.006571</td><td>0.51887</td><td>-0.344441</td><td>0.641694</td><td>-0.64604</td><td>-0.50626</td><td>0.739797</td><td>-2.041514</td></tr><tr><td>3</td><td>1530</td><td>0</td><td>3</td><td>1.986533</td><td>0.647099</td><td>1.68746</td><td>0.569406</td><td>1.061679</td><td>2.444131</td><td>0.150487</td><td>0.896543</td><td>0.705652</td><td>0.072545</td><td>4</td><td>3</td><td>11</td><td>-0.966606</td><td>0.415421</td><td>-0.528979</td><td>null</td><td>0.183964</td><td>null</td><td>-1.917427</td><td>-1.929856</td><td>-0.50175</td><td>-0.102371</td><td>-0.297677</td><td>-1.049185</td><td>0.933516</td><td>-0.60654</td><td>-0.039158</td><td>-0.27196</td><td>-0.920819</td><td>-0.926502</td><td>-0.783003</td><td>-0.13572</td><td>&hellip;</td><td>2.07314</td><td>null</td><td>null</td><td>0.205406</td><td>null</td><td>-0.760486</td><td>2.06341</td><td>null</td><td>4.528522</td><td>1.582996</td><td>0.385893</td><td>-0.396047</td><td>-0.146214</td><td>-0.228367</td><td>-2.062314</td><td>-1.421811</td><td>-0.851179</td><td>0.404792</td><td>-0.509694</td><td>-1.398495</td><td>0.231285</td><td>-0.595247</td><td>null</td><td>null</td><td>6.995454</td><td>7.133929</td><td>0.352965</td><td>0.526284</td><td>-0.349773</td><td>-0.235901</td><td>-0.428956</td><td>-1.903627</td><td>-1.214619</td><td>-0.4695</td><td>-2.590589</td><td>-0.946317</td><td>-0.390001</td></tr><tr><td>4</td><td>1530</td><td>0</td><td>4</td><td>2.742601</td><td>1.096778</td><td>1.551411</td><td>0.632113</td><td>0.368218</td><td>2.181873</td><td>0.214604</td><td>1.19423</td><td>0.261056</td><td>0.045426</td><td>15</td><td>1</td><td>9</td><td>-0.895004</td><td>0.047908</td><td>-0.683484</td><td>null</td><td>-0.638341</td><td>null</td><td>-1.55811</td><td>-1.203378</td><td>-0.305831</td><td>0.251725</td><td>0.222749</td><td>-0.902276</td><td>4.226925</td><td>1.233024</td><td>-2.274672</td><td>-1.065931</td><td>-0.229211</td><td>-0.597557</td><td>-0.990242</td><td>0.173607</td><td>&hellip;</td><td>0.549144</td><td>null</td><td>null</td><td>0.383838</td><td>null</td><td>-1.490257</td><td>2.019752</td><td>null</td><td>1.872912</td><td>0.571886</td><td>0.385893</td><td>-0.445097</td><td>-0.347954</td><td>-0.470278</td><td>-1.780053</td><td>-2.290865</td><td>-0.944827</td><td>0.027896</td><td>-0.522902</td><td>-1.123492</td><td>0.120676</td><td>-0.610595</td><td>null</td><td>null</td><td>0.493589</td><td>0.560821</td><td>-1.332615</td><td>-0.965623</td><td>-0.373938</td><td>-0.209282</td><td>-0.095182</td><td>-1.598217</td><td>0.968505</td><td>-0.705594</td><td>-1.579623</td><td>0.954296</td><td>-1.805623</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>6274571</td><td>1698</td><td>967</td><td>34</td><td>3.242493</td><td>2.52516</td><td>-0.721981</td><td>2.544025</td><td>2.477615</td><td>0.417557</td><td>0.785812</td><td>1.117796</td><td>2.199436</td><td>0.415427</td><td>42</td><td>5</td><td>150</td><td>0.804403</td><td>1.157257</td><td>1.031543</td><td>-0.671189</td><td>-0.3286</td><td>-0.486132</td><td>1.730176</td><td>-0.006173</td><td>-0.001144</td><td>-0.213062</td><td>0.932618</td><td>1.367338</td><td>-0.238197</td><td>-0.692615</td><td>-0.121163</td><td>1.090798</td><td>1.444294</td><td>-0.675626</td><td>-1.013264</td><td>-0.242888</td><td>&hellip;</td><td>0.057455</td><td>0.953005</td><td>1.377051</td><td>-0.396358</td><td>0.520262</td><td>1.179617</td><td>1.127657</td><td>2.231928</td><td>0.614652</td><td>2.412886</td><td>-1.101531</td><td>-0.384833</td><td>-0.275818</td><td>-0.40804</td><td>2.427115</td><td>-0.108427</td><td>0.739734</td><td>0.830205</td><td>0.366287</td><td>1.33325</td><td>1.075499</td><td>1.798264</td><td>-0.183443</td><td>-0.190222</td><td>0.234211</td><td>0.347142</td><td>-0.044463</td><td>0.016936</td><td>0.243475</td><td>0.166927</td><td>0.38494</td><td>-0.174297</td><td>-0.066046</td><td>-0.038767</td><td>-0.132337</td><td>-0.022426</td><td>-0.252461</td></tr><tr><td>6274572</td><td>1698</td><td>967</td><td>35</td><td>1.079139</td><td>1.857906</td><td>-0.790646</td><td>2.745439</td><td>2.339877</td><td>0.845065</td><td>0.65137</td><td>1.180301</td><td>1.966379</td><td>0.321543</td><td>25</td><td>7</td><td>195</td><td>-0.075294</td><td>-0.152726</td><td>-0.20417</td><td>-0.421137</td><td>0.21708</td><td>-0.258775</td><td>1.874978</td><td>0.19988</td><td>-0.199219</td><td>-0.125619</td><td>-1.004547</td><td>-0.051933</td><td>0.450905</td><td>0.009246</td><td>0.164127</td><td>-0.939974</td><td>-1.143421</td><td>-0.320071</td><td>-0.379835</td><td>-0.142429</td><td>&hellip;</td><td>0.219276</td><td>-0.315776</td><td>0.687755</td><td>-1.189577</td><td>0.180146</td><td>-0.175486</td><td>-1.60435</td><td>-0.209283</td><td>0.249847</td><td>0.288816</td><td>-1.101531</td><td>-0.343868</td><td>-0.253991</td><td>-0.278832</td><td>2.050639</td><td>-0.059506</td><td>-0.029396</td><td>-0.101381</td><td>-0.187759</td><td>-0.180839</td><td>-0.0861</td><td>-0.153405</td><td>-0.196077</td><td>-0.175292</td><td>1.04578</td><td>0.739733</td><td>0.03372</td><td>0.05086</td><td>0.850152</td><td>0.909382</td><td>1.015314</td><td>0.235962</td><td>0.122539</td><td>0.099559</td><td>-0.249584</td><td>-0.123571</td><td>-0.46063</td></tr><tr><td>6274573</td><td>1698</td><td>967</td><td>36</td><td>1.033172</td><td>2.515527</td><td>-0.672298</td><td>2.28925</td><td>2.521592</td><td>0.255077</td><td>0.919892</td><td>1.172018</td><td>2.180496</td><td>0.24846</td><td>49</td><td>7</td><td>297</td><td>1.026715</td><td>-0.096892</td><td>0.224309</td><td>-0.528109</td><td>-0.704952</td><td>-0.704818</td><td>2.312482</td><td>0.32804</td><td>-0.108193</td><td>null</td><td>-0.945684</td><td>-0.244173</td><td>0.205989</td><td>-0.357343</td><td>null</td><td>null</td><td>-1.11075</td><td>-0.580242</td><td>-0.400568</td><td>null</td><td>&hellip;</td><td>-0.549192</td><td>1.338474</td><td>0.933568</td><td>0.032978</td><td>-0.519118</td><td>-0.290343</td><td>-0.806786</td><td>0.106295</td><td>0.183461</td><td>1.830421</td><td>-1.101531</td><td>-0.341991</td><td>-0.249132</td><td>-0.34365</td><td>2.251358</td><td>0.601888</td><td>1.035051</td><td>-0.283241</td><td>0.107244</td><td>0.86016</td><td>0.024223</td><td>0.374852</td><td>-0.220933</td><td>-0.161584</td><td>0.032771</td><td>0.036888</td><td>0.168908</td><td>0.152333</td><td>0.395684</td><td>-0.292574</td><td>-3.215846</td><td>-0.535129</td><td>-0.178484</td><td>-1.80815</td><td>-0.065355</td><td>-0.000367</td><td>-0.12517</td></tr><tr><td>6274574</td><td>1698</td><td>967</td><td>37</td><td>1.243116</td><td>2.663298</td><td>-0.889112</td><td>2.313155</td><td>3.101428</td><td>0.324454</td><td>0.618944</td><td>1.185663</td><td>1.599724</td><td>0.319719</td><td>34</td><td>4</td><td>214</td><td>0.759314</td><td>0.284057</td><td>0.41716</td><td>-0.611075</td><td>-0.513717</td><td>-0.891423</td><td>1.84994</td><td>0.406756</td><td>-1.608196</td><td>-0.252663</td><td>-0.271574</td><td>-0.051405</td><td>0.098146</td><td>-0.653961</td><td>0.173676</td><td>-0.016497</td><td>-0.404509</td><td>-0.577262</td><td>-0.731429</td><td>-0.21646</td><td>&hellip;</td><td>0.352567</td><td>0.471775</td><td>1.876459</td><td>-0.143377</td><td>0.845516</td><td>0.301135</td><td>-0.395703</td><td>0.738038</td><td>-0.04124</td><td>1.270645</td><td>-1.101531</td><td>-0.358106</td><td>-0.141883</td><td>-0.255192</td><td>2.489247</td><td>0.537652</td><td>0.982107</td><td>-0.158009</td><td>0.137389</td><td>0.478357</td><td>0.782692</td><td>0.581421</td><td>-0.106056</td><td>-0.111017</td><td>0.163867</td><td>0.169331</td><td>-0.037563</td><td>-0.029483</td><td>1.925987</td><td>0.479394</td><td>3.621867</td><td>-0.107114</td><td>-0.063599</td><td>1.204755</td><td>-0.148711</td><td>-0.026583</td><td>-0.256395</td></tr><tr><td>6274575</td><td>1698</td><td>967</td><td>38</td><td>3.193685</td><td>2.728506</td><td>-0.745238</td><td>2.788789</td><td>2.343393</td><td>0.454731</td><td>0.862839</td><td>0.964795</td><td>2.089673</td><td>0.344931</td><td>50</td><td>1</td><td>522</td><td>0.406531</td><td>0.618247</td><td>1.01327</td><td>-0.952069</td><td>-0.679168</td><td>-0.597603</td><td>0.375125</td><td>1.97537</td><td>-0.440974</td><td>-0.072018</td><td>1.741353</td><td>1.380735</td><td>-0.110494</td><td>-0.874806</td><td>0.553424</td><td>0.532243</td><td>0.263214</td><td>-0.757856</td><td>-0.869204</td><td>-0.062955</td><td>&hellip;</td><td>0.263278</td><td>0.915804</td><td>1.862022</td><td>0.503819</td><td>1.310126</td><td>0.662521</td><td>1.654948</td><td>1.090367</td><td>0.535922</td><td>0.653011</td><td>-1.101531</td><td>-0.622853</td><td>-0.363631</td><td>-0.395652</td><td>-0.016812</td><td>2.016734</td><td>0.241486</td><td>0.253229</td><td>0.228745</td><td>0.462717</td><td>0.799635</td><td>0.706102</td><td>-0.376377</td><td>-0.286764</td><td>-0.359046</td><td>-0.246135</td><td>-0.288941</td><td>-0.247774</td><td>1.228778</td><td>0.512562</td><td>-0.050865</td><td>0.160883</td><td>0.080756</td><td>-0.078237</td><td>-0.138548</td><td>-0.038771</td><td>-0.21194</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (6_274_576, 93)\n",
       "┌─────────┬─────────┬─────────┬───────────┬───┬─────────────┬────────────┬────────────┬────────────┐\n",
       "│ row_id  ┆ date_id ┆ time_id ┆ symbol_id ┆ … ┆ responder_5 ┆ responder_ ┆ responder_ ┆ responder_ │\n",
       "│ ---     ┆ ---     ┆ ---     ┆ ---       ┆   ┆ ---         ┆ 6          ┆ 7          ┆ 8          │\n",
       "│ u32     ┆ i16     ┆ i16     ┆ i8        ┆   ┆ f32         ┆ ---        ┆ ---        ┆ ---        │\n",
       "│         ┆         ┆         ┆           ┆   ┆             ┆ f32        ┆ f32        ┆ f32        │\n",
       "╞═════════╪═════════╪═════════╪═══════════╪═══╪═════════════╪════════════╪════════════╪════════════╡\n",
       "│ 0       ┆ 1530    ┆ 0       ┆ 0         ┆ … ┆ 0.552013    ┆ 3.071231   ┆ 0.914794   ┆ 0.997124   │\n",
       "│ 1       ┆ 1530    ┆ 0       ┆ 1         ┆ … ┆ 0.757289    ┆ 1.979042   ┆ 0.967537   ┆ 1.219739   │\n",
       "│ 2       ┆ 1530    ┆ 0       ┆ 2         ┆ … ┆ -0.64604    ┆ -0.50626   ┆ 0.739797   ┆ -2.041514  │\n",
       "│ 3       ┆ 1530    ┆ 0       ┆ 3         ┆ … ┆ -0.4695     ┆ -2.590589  ┆ -0.946317  ┆ -0.390001  │\n",
       "│ 4       ┆ 1530    ┆ 0       ┆ 4         ┆ … ┆ -0.705594   ┆ -1.579623  ┆ 0.954296   ┆ -1.805623  │\n",
       "│ …       ┆ …       ┆ …       ┆ …         ┆ … ┆ …           ┆ …          ┆ …          ┆ …          │\n",
       "│ 6274571 ┆ 1698    ┆ 967     ┆ 34        ┆ … ┆ -0.038767   ┆ -0.132337  ┆ -0.022426  ┆ -0.252461  │\n",
       "│ 6274572 ┆ 1698    ┆ 967     ┆ 35        ┆ … ┆ 0.099559    ┆ -0.249584  ┆ -0.123571  ┆ -0.46063   │\n",
       "│ 6274573 ┆ 1698    ┆ 967     ┆ 36        ┆ … ┆ -1.80815    ┆ -0.065355  ┆ -0.000367  ┆ -0.12517   │\n",
       "│ 6274574 ┆ 1698    ┆ 967     ┆ 37        ┆ … ┆ 1.204755    ┆ -0.148711  ┆ -0.026583  ┆ -0.256395  │\n",
       "│ 6274575 ┆ 1698    ┆ 967     ┆ 38        ┆ … ┆ -0.078237   ┆ -0.138548  ┆ -0.038771  ┆ -0.21194   │\n",
       "└─────────┴─────────┴─────────┴───────────┴───┴─────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds = pl.scan_parquet('/home/lorecampa/projects/jane_street_forecasting/dataset/train.parquet/partition_id=9/part-0.parquet').collect().with_row_index('row_id')\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008284926414489746"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "X_test = test_ds.select([f'feature_{i:02d}' for i in range(79)]).fill_null(strategy='zero').to_numpy()\n",
    "y_test = test_ds['responder_6'].to_numpy().flatten()\n",
    "w_test = test_ds['weight'].to_numpy().flatten()\n",
    "with torch.no_grad():\n",
    "    y_test_hat = inference_model(torch.tensor(X_test, dtype=torch.float32).to(inference_model.device)).cpu().numpy().flatten()\n",
    "    \n",
    "r2_score(y_true=y_test, y_pred=y_test_hat, sample_weight=w_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "\n",
    "del X_test, y_test, w_test, y_test_hat\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def online_iterator(df: pl.DataFrame):\n",
    "    assert df.select('date_id').n_unique() > 1, 'Dataset must contain at least 2 days'\n",
    "    \n",
    "    df_date_time_id = df.select('date_id', 'time_id').unique().sort('date_id', 'time_id').with_row_index('date_time_id')\n",
    "    df = df.join(df_date_time_id, on=['date_id', 'time_id'], how='left', maintain_order='left')\n",
    "    \n",
    "    max_date_time_id = df_date_time_id['date_time_id'].max()\n",
    "    min_date_id = df.select('date_id').min().item()\n",
    "    \n",
    "    responders = [f'responder_{i}' for i in range(9)]\n",
    "    \n",
    "    curr_idx:int = df_date_time_id.filter(pl.col('date_id').eq(min_date_id + 1))['date_time_id'].min()\n",
    "    old_day = min_date_id\n",
    "\n",
    "    with tqdm(total=max_date_time_id - curr_idx + 1) as pbar:\n",
    "        while curr_idx <= max_date_time_id:\n",
    "            curr_day = df_date_time_id[curr_idx]['date_id'].item()\n",
    "            is_new_day = curr_day != old_day\n",
    "            lags = None\n",
    "            if is_new_day:\n",
    "                print('new day')\n",
    "                lags = df.filter(pl.col('date_id').eq(old_day)).select(pl.col('date_id').add(1), 'time_id', 'symbol_id', *[pl.col(r).alias(f'{r}_lag_1') for r in responders])\n",
    "            \n",
    "            old_day = curr_day\n",
    "\n",
    "            batch = df.filter(pl.col('date_time_id').eq(curr_idx)).with_columns(pl.lit(True).alias('is_scored')).drop('date_time_id')\n",
    "            \n",
    "            yield batch, lags if lags is not None else None\n",
    "            \n",
    "            curr_idx += 1\n",
    "            pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/162624 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new day\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for test_batch, lags in online_iterator(test_ds):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New day\n",
      "Completed Fine Tuning at time 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (39, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>row_id</th><th>responder_6</th></tr><tr><td>u32</td><td>f32</td></tr></thead><tbody><tr><td>35816</td><td>0.127188</td></tr><tr><td>35817</td><td>0.062084</td></tr><tr><td>35818</td><td>-0.043785</td></tr><tr><td>35819</td><td>-0.090471</td></tr><tr><td>35820</td><td>-0.128343</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>35850</td><td>0.003515</td></tr><tr><td>35851</td><td>0.097004</td></tr><tr><td>35852</td><td>0.055536</td></tr><tr><td>35853</td><td>-0.056201</td></tr><tr><td>35854</td><td>0.1147</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (39, 2)\n",
       "┌────────┬─────────────┐\n",
       "│ row_id ┆ responder_6 │\n",
       "│ ---    ┆ ---         │\n",
       "│ u32    ┆ f32         │\n",
       "╞════════╪═════════════╡\n",
       "│ 35816  ┆ 0.127188    │\n",
       "│ 35817  ┆ 0.062084    │\n",
       "│ 35818  ┆ -0.043785   │\n",
       "│ 35819  ┆ -0.090471   │\n",
       "│ 35820  ┆ -0.128343   │\n",
       "│ …      ┆ …           │\n",
       "│ 35850  ┆ 0.003515    │\n",
       "│ 35851  ┆ 0.097004    │\n",
       "│ 35852  ┆ 0.055536    │\n",
       "│ 35853  ┆ -0.056201   │\n",
       "│ 35854  ┆ 0.1147      │\n",
       "└────────┴─────────────┘"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(test_batch, lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

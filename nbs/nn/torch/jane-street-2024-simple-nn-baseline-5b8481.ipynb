{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-15T23:05:46.920162Z",
     "iopub.status.busy": "2024-12-15T23:05:46.919835Z",
     "iopub.status.idle": "2024-12-15T23:05:53.224477Z",
     "shell.execute_reply": "2024-12-15T23:05:53.223586Z",
     "shell.execute_reply.started": "2024-12-15T23:05:46.920131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import gc\n",
    "import os\n",
    "from typing import List, Union, Dict, Any\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchmetrics.functional as tmf\n",
    "import lightning as L\n",
    "import lightning.pytorch.callbacks as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:05:53.226068Z",
     "iopub.status.busy": "2024-12-15T23:05:53.225578Z",
     "iopub.status.idle": "2024-12-15T23:05:53.254232Z",
     "shell.execute_reply": "2024-12-15T23:05:53.253251Z",
     "shell.execute_reply.started": "2024-12-15T23:05:53.226030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from prj.config import DATA_DIR\n",
    "\n",
    "\n",
    "BASE_PATH = DATA_DIR\n",
    "\n",
    "train_ds = pl.concat([\n",
    "    pl.scan_parquet(BASE_PATH / f'partition_id={i}' / 'part-0.parquet')\n",
    "    for i in range(8, 9)\n",
    "])\n",
    "val_ds = pl.scan_parquet(BASE_PATH / 'partition_id=9' / 'part-0.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:05:53.268363Z",
     "iopub.status.busy": "2024-12-15T23:05:53.267966Z",
     "iopub.status.idle": "2024-12-15T23:05:53.279761Z",
     "shell.execute_reply": "2024-12-15T23:05:53.279012Z",
     "shell.execute_reply.started": "2024-12-15T23:05:53.268327Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class JaneStreetBaseDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, dataset: pl.LazyFrame, num_days_batch: int = 10, \n",
    "                 num_stocks: int = 39, num_timesteps: int = 50):\n",
    "        super(JaneStreetBaseDataset, self).__init__()   \n",
    "        self.dataset = dataset\n",
    "        self.num_days_batch = num_days_batch\n",
    "        self.num_stocks = num_stocks\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.dataset_len = self.dataset.select(['date_id', 'time_id', 'symbol_id']).unique().collect().shape[0]\n",
    "        self._load()\n",
    "    \n",
    "    def _shuffle_batches(self):\n",
    "        np.random.shuffle(self.dates)\n",
    "    \n",
    "    def _load(self):\n",
    "        feature_cols = [f'feature_{i:02d}' for i in range(79)]\n",
    "        preprocessed_dataset = (\n",
    "            self.dataset\n",
    "            .sort(['date_id', 'time_id'])\n",
    "            .with_columns(pl.col(feature_cols).fill_null(strategy='forward', limit=10).over('symbol_id').fill_null(strategy='zero'))\n",
    "        )\n",
    "        \n",
    "        self.X = preprocessed_dataset.select(feature_cols).collect().to_numpy().astype(np.float32)\n",
    "        self.y = preprocessed_dataset.select(['responder_6']).collect().to_numpy().flatten().astype(np.float32)\n",
    "        self.weights = preprocessed_dataset.select(['weight']).collect().to_numpy().flatten().astype(np.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    "    \n",
    "    def __getitem__(self, idx):       \n",
    "        return (\n",
    "            torch.tensor(self.X[idx, :], dtype=torch.float32), \n",
    "            torch.tensor(self.y[idx], dtype=torch.float32), \n",
    "            torch.tensor(self.weights[idx], dtype=torch.float32)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:05:53.280832Z",
     "iopub.status.busy": "2024-12-15T23:05:53.280594Z",
     "iopub.status.idle": "2024-12-15T23:08:21.421845Z",
     "shell.execute_reply": "2024-12-15T23:08:21.420855Z",
     "shell.execute_reply.started": "2024-12-15T23:05:53.280808Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = JaneStreetBaseDataset(train_ds, num_days_batch=200)\n",
    "val_dataset = JaneStreetBaseDataset(val_ds, num_days_batch=200)\n",
    "\n",
    "batch_size = 1024\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:08:21.423180Z",
     "iopub.status.busy": "2024-12-15T23:08:21.422938Z",
     "iopub.status.idle": "2024-12-15T23:08:21.427645Z",
     "shell.execute_reply": "2024-12-15T23:08:21.426762Z",
     "shell.execute_reply.started": "2024-12-15T23:08:21.423158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def weighted_r2_score(preds, targets, weights):\n",
    "    ss_res = (weights * (targets - preds) ** 2).sum()\n",
    "    ss_tot = (weights * (targets ** 2)).sum()\n",
    "    return 1 - ss_res / ss_tot if ss_tot > 0 else 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:08:21.429092Z",
     "iopub.status.busy": "2024-12-15T23:08:21.428784Z",
     "iopub.status.idle": "2024-12-15T23:08:21.448993Z",
     "shell.execute_reply": "2024-12-15T23:08:21.448163Z",
     "shell.execute_reply.started": "2024-12-15T23:08:21.429068Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class JaneStreetBaseModel(L.LightningModule):\n",
    "\n",
    "    def __init__(self, \n",
    "                 model: nn.Module,\n",
    "                 losses: List[nn.Module] | nn.Module, \n",
    "                 loss_weights: List[float], \n",
    "                 l1_lambda: float = 1e-4,\n",
    "                 l2_lambda: float = 1e-4,\n",
    "                 optimizer: str = 'Adam',\n",
    "                 optimizer_cfg: Dict[str, Any] = dict(),\n",
    "                 scheduler: str = None,\n",
    "                 scheduler_cfg: Dict[str, Any] = dict()):\n",
    "        super(JaneStreetBaseModel, self).__init__()   \n",
    "        assert isinstance(losses, nn.Module) or len(losses) == len(loss_weights), 'Each loss must have a weight'\n",
    "        assert len(loss_weights) == 0 or min(loss_weights) > 0, 'Losses must have positive weights'\n",
    "        self.model = model\n",
    "        losses = [losses] if isinstance(losses, nn.Module) else losses\n",
    "        self.losses = nn.ModuleList(losses) \n",
    "        self.loss_weights = [1.0] if isinstance(losses, nn.Module) else loss_weights\n",
    "        self.l1_lambda = l1_lambda\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.optimizer_name = optimizer\n",
    "        self.optimizer_cfg = optimizer_cfg\n",
    "        self.scheduler_name = scheduler\n",
    "        self.scheduler_cfg = scheduler_cfg\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y, weights = batch\n",
    "        y_hat = self.forward(x).squeeze()\n",
    "        loss = self._compute_loss(y_hat, y, weights)\n",
    "        reg_loss, l1_loss, l2_loss = self._regularization_loss()\n",
    "        with torch.no_grad():\n",
    "            metrics = self._compute_metrics(y_hat, y, weights, prefix='train')\n",
    "        metrics['train_loss'] = loss\n",
    "        metrics['train_l1_reg'] = l1_loss\n",
    "        metrics['train_l2_reg'] = l2_loss\n",
    "        self.log_dict(metrics, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss + reg_loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y, weights = batch\n",
    "        y_hat = self.forward(x).squeeze()\n",
    "        loss = self._compute_loss(y_hat, y, weights)\n",
    "        metrics = self._compute_metrics(y_hat, y, weights, prefix='val')\n",
    "        metrics['val_loss'] = loss\n",
    "        self.log_dict(metrics, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def _compute_metrics(self, preds, targets, weights, prefix='val'):\n",
    "        metrics = dict()\n",
    "        metrics[f'{prefix}_wmse'] = (weights * (preds - targets) ** 2).sum() / weights.sum()\n",
    "        metrics[f'{prefix}_wmae'] = (weights * (preds - targets).abs()).sum() / weights.sum()\n",
    "        metrics[f'{prefix}_wr2'] = weighted_r2_score(preds, targets, weights)\n",
    "        return metrics\n",
    "\n",
    "    def _compute_loss(self, preds, targets, weights):\n",
    "        loss = 0\n",
    "        for i in range(len(self.losses)):\n",
    "            loss += self.losses[i](preds, targets, weights=weights) * self.loss_weights[i]\n",
    "        return loss\n",
    "    \n",
    "    def _regularization_loss(self):\n",
    "        reg_loss = 0\n",
    "        if self.l1_lambda > 0:\n",
    "            l1_loss = sum(p.abs().sum() for p in self.parameters())\n",
    "            reg_loss += l1_loss * self.l1_lambda\n",
    "            \n",
    "        if self.l2_lambda > 0:\n",
    "            l2_loss = sum(p.pow(2).sum() for p in self.parameters())\n",
    "            reg_loss += l2_loss * self.l2_lambda\n",
    "            \n",
    "        return reg_loss, l1_loss, l2_loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = getattr(torch.optim, self.optimizer_name)(self.parameters(), **self.optimizer_cfg)\n",
    "        if self.scheduler_name is None:\n",
    "            return optimizer\n",
    "        scheduler = getattr(torch.optim.lr_scheduler, self.scheduler_name)(optimizer, **self.scheduler_cfg)\n",
    "        return [optimizer], [{'scheduler': scheduler, 'monitor': 'val_wr2'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:08:21.450115Z",
     "iopub.status.busy": "2024-12-15T23:08:21.449842Z",
     "iopub.status.idle": "2024-12-15T23:08:21.466468Z",
     "shell.execute_reply": "2024-12-15T23:08:21.465846Z",
     "shell.execute_reply.started": "2024-12-15T23:08:21.450086Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class WeightedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WeightedMSELoss, self).__init__()\n",
    "    \n",
    "    def forward(self, predictions: Tensor, targets: Tensor, weights: Tensor) -> Tensor:\n",
    "        squared_diff = (predictions - targets) ** 2\n",
    "        weighted_squared_diff = weights * squared_diff\n",
    "        return weighted_squared_diff.sum() / weights.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:08:21.468841Z",
     "iopub.status.busy": "2024-12-15T23:08:21.468551Z",
     "iopub.status.idle": "2024-12-15T23:08:21.478673Z",
     "shell.execute_reply": "2024-12-15T23:08:21.478009Z",
     "shell.execute_reply.started": "2024-12-15T23:08:21.468803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SimpleNNModel(nn.Module):\n",
    "    def __init__(self, input_features, hidden_dims=[], use_dropout=True, \n",
    "                 dropout_rate=0.1, use_bn=True, output_dim=1, use_tanh=False, final_mult=1.0):\n",
    "        super(SimpleNNModel, self).__init__()\n",
    "        self.final_mult = final_mult\n",
    "        self.use_tanh = use_tanh\n",
    "        \n",
    "        layers = []\n",
    "        in_features = input_features\n",
    "\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_features, hidden_dim))\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.LeakyReLU())\n",
    "            if use_dropout:\n",
    "                layers.append(nn.Dropout(dropout_rate))\n",
    "            in_features = hidden_dim\n",
    "            \n",
    "        layers.append(nn.Linear(in_features, output_dim))\n",
    "        if self.use_tanh:\n",
    "            layers.append(nn.Tanh())\n",
    "        \n",
    "        self.model = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.final_mult * self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:08:21.479812Z",
     "iopub.status.busy": "2024-12-15T23:08:21.479568Z",
     "iopub.status.idle": "2024-12-15T23:08:21.489669Z",
     "shell.execute_reply": "2024-12-15T23:08:21.488888Z",
     "shell.execute_reply.started": "2024-12-15T23:08:21.479789Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EpochStatsCallback(C.Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics  # Get all logged metrics\n",
    "        train_metrics = {k: v.item() for k, v in metrics.items() if k.startswith(\"train_\")}\n",
    "        epoch = trainer.current_epoch\n",
    "        print(f\"\\n[Epoch {epoch} - Training]\")\n",
    "        for key, value in train_metrics.items():\n",
    "            print(f\"{key}: {value:.4f}\")\n",
    "\n",
    "    def on_validation_epoch_end(self, trainer, pl_module):\n",
    "        metrics = trainer.callback_metrics  # Get all logged metrics\n",
    "        val_metrics = {k: v.item() for k, v in metrics.items() if k.startswith(\"val_\")}\n",
    "        epoch = trainer.current_epoch\n",
    "        print(f\"\\n[Epoch {epoch} - Validation]\")\n",
    "        for key, value in val_metrics.items():\n",
    "            print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:08:21.490701Z",
     "iopub.status.busy": "2024-12-15T23:08:21.490467Z",
     "iopub.status.idle": "2024-12-15T23:08:21.504395Z",
     "shell.execute_reply": "2024-12-15T23:08:21.503690Z",
     "shell.execute_reply.started": "2024-12-15T23:08:21.490657Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(model: L.LightningModule,\n",
    "          train_dataloader: DataLoader,\n",
    "          val_dataloader: DataLoader,\n",
    "          checkpoint_path: Union[str, Path] = None,\n",
    "          max_epochs: int = 1000,\n",
    "          eval_frequency: int = 1,\n",
    "          log_every_n_steps: int = 10,\n",
    "          precision: str = \"16-mixed\",\n",
    "          accumulate_grad_batches: int = 1,\n",
    "          gradient_clip_val: int = 10,\n",
    "          gradient_clip_algorithm: str = 'norm',\n",
    "          use_swa : bool = False,\n",
    "          swa_cfg: Dict[str, Any] = None,\n",
    "          use_early_stopping: bool = False,\n",
    "          early_stopping_cfg: Dict[str, Any] = None,\n",
    "          use_model_ckpt: bool = True,\n",
    "          model_ckpt_cfg: Dict[str, Any] = None,\n",
    "          seed: int = 42,\n",
    "          compile: bool = False):\n",
    "    if compile:\n",
    "        model = torch.compile(model, fullgraph=False, dynamic=False)\n",
    "\n",
    "    L.seed_everything(seed, workers=True)\n",
    "\n",
    "    callbacks = [C.ModelSummary(max_depth=3), \n",
    "                 C.LearningRateMonitor(logging_interval='epoch'),\n",
    "                 EpochStatsCallback()]\n",
    "    if use_swa:\n",
    "        callbacks.append(C.StochasticWeightAveraging(**swa_cfg))\n",
    "    if use_early_stopping:\n",
    "        callbacks.append(C.EarlyStopping(**early_stopping_cfg))\n",
    "    if use_model_ckpt:\n",
    "        callbacks.append(C.ModelCheckpoint(**model_ckpt_cfg))\n",
    "\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        check_val_every_n_epoch=eval_frequency,\n",
    "        log_every_n_steps=log_every_n_steps,\n",
    "        precision=precision,\n",
    "        accumulate_grad_batches=accumulate_grad_batches,\n",
    "        gradient_clip_val=gradient_clip_val,\n",
    "        gradient_clip_algorithm=gradient_clip_algorithm,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    trainer.fit(model=model, train_dataloaders=train_dataloader,\n",
    "                val_dataloaders=val_dataloader, ckpt_path=checkpoint_path)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T23:08:21.505770Z",
     "iopub.status.busy": "2024-12-15T23:08:21.505399Z",
     "iopub.status.idle": "2024-12-15T23:34:38.870773Z",
     "shell.execute_reply": "2024-12-15T23:34:38.870022Z",
     "shell.execute_reply.started": "2024-12-15T23:08:21.505734Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from prj.config import EXP_DIR\n",
    "\n",
    "\n",
    "scheduler = 'ReduceLROnPlateau'\n",
    "scheduler_cfg = dict(mode='max', factor=0.1, patience=3, verbose=True, min_lr=1e-8)\n",
    "\n",
    "\n",
    "base_model = SimpleNNModel(79, hidden_dims=[128, 128], dropout_rate=0.1, final_mult=5.0, use_tanh=True)\n",
    "model = JaneStreetBaseModel(base_model, [WeightedMSELoss()], [1], l1_lambda=1e-6, \n",
    "                            l2_lambda=1e-4, scheduler=scheduler, scheduler_cfg=scheduler_cfg)\n",
    "\n",
    "\n",
    "data_dir = str(EXP_DIR / 'tmp' / 'model1')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "ckpt_config = {'dirpath': data_dir, 'filename': 'mlp', 'save_top_k': 1,\n",
    "               'monitor': 'val_wr2', 'verbose': True, 'mode': 'max'}\n",
    "early_stopping = {'monitor': 'val_wr2', 'min_delta': 0.00, 'patience': 5, 'verbose': True, 'mode': 'max'}\n",
    "swa_config = {'swa_lrs': 0.05, 'swa_epoch_start': 4}\n",
    "model = train(model, train_dataloader, val_dataloader, max_epochs=10, precision='32-true', \n",
    "              use_model_ckpt=True, gradient_clip_val=10, use_early_stopping=True, \n",
    "              early_stopping_cfg=early_stopping, model_ckpt_cfg=ckpt_config, \n",
    "              use_swa=False, swa_cfg=swa_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = SimpleNNModel(79, hidden_dims=[128, 128], dropout_rate=0.1, final_mult=5.0, use_tanh=True)\n",
    "model = JaneStreetBaseModel.load_from_checkpoint(\n",
    "    f\"{data_dir}/mlp.ckpt\", \n",
    "    model=base_model,\n",
    "    losses=[WeightedMSELoss()],\n",
    "    loss_weights=[1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import r2_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "val_dataset = JaneStreetBaseDataset(val_ds, num_days_batch=50)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=2048, shuffle=False)\n",
    "\n",
    "y_hat = []\n",
    "y = []\n",
    "weights = []\n",
    "\n",
    "model.eval()\n",
    "for x, targets, w in tqdm(iter(val_dataloader)):\n",
    "    x = x.to(device)\n",
    "    targets = targets.to(device)\n",
    "    w = w.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds_all = model(x)\n",
    "\n",
    "    y_hat.append(preds_all.cpu().numpy().flatten())\n",
    "    y.append(targets.cpu().numpy().flatten())\n",
    "    weights.append(w.cpu().numpy().flatten())\n",
    "\n",
    "y = np.concatenate(y)\n",
    "y_hat = np.concatenate(y_hat)\n",
    "weights = np.concatenate(weights)\n",
    "\n",
    "(\n",
    "    weighted_r2_score(y_hat, y, weights)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-12-15T23:35:10.132783Z",
     "iopub.status.idle": "2024-12-15T23:35:10.133078Z",
     "shell.execute_reply": "2024-12-15T23:35:10.132953Z",
     "shell.execute_reply.started": "2024-12-15T23:35:10.132937Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 9871156,
     "sourceId": 84493,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
